{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":30732,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install -U nbformat --no-cache-dir","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!pip install -U triton --no-cache-dir","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!pip install -U accelerate --no-cache-dir","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%capture\n# Installs Unsloth, Xformers (Flash Attention) and all other packages!\n!pip install \"unsloth[colab-new] @ git+https://github.com/unslothai/unsloth.git\"\n!pip install --no-deps \"trl<0.9.0\" peft accelerate bitsandbytes --no-cache-dir","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!pip install -U xformers --no-cache-dir","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!pip install -U torch torchvision torchaudio --no-cache-dir","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import torch\nprint(torch.cuda.is_available())","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!nvcc --version","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!export OMP_NUM_THREADS=1\n!export MKL_NUM_THREADS=1","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from unsloth import FastLanguageModel\nimport torch\nmax_seq_length = 2048 # Choose any! We auto support RoPE Scaling internally!\ndtype = None # None for auto detection. Float16 for Tesla T4, V100, Bfloat16 for Ampere+\nload_in_4bit = True # Use 4bit quantization to reduce memory usage. Can be False.\n\n# 4bit pre quantized models we support for 4x faster downloading + no OOMs.\nfourbit_models = [\n    \"unsloth/mistral-7b-v0.3-bnb-4bit\",      # New Mistral v3 2x faster!\n    \"unsloth/mistral-7b-instruct-v0.3-bnb-4bit\",\n    \"unsloth/llama-3-8b-bnb-4bit\",           # Llama-3 15 trillion tokens model 2x faster!\n    \"unsloth/llama-3-8b-Instruct-bnb-4bit\",\n    \"unsloth/llama-3-70b-bnb-4bit\",\n    \"unsloth/Phi-3-mini-4k-instruct\",        # Phi-3 2x faster!\n    \"unsloth/Phi-3-medium-4k-instruct\",\n    \"unsloth/mistral-7b-bnb-4bit\",\n    \"unsloth/gemma-7b-bnb-4bit\",             # Gemma 2.2x faster!\n] # More models at https://huggingface.co/unsloth\n\nmodel, tokenizer = FastLanguageModel.from_pretrained(\n    model_name = \"meta-llama/Meta-Llama-3-8B\",\n    max_seq_length = max_seq_length,\n    dtype = dtype,\n    load_in_4bit = load_in_4bit,\n    token = \"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\", # use one if using gated models like meta-llama/Llama-2-7b-hf\n)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model = FastLanguageModel.get_peft_model(\n    model,\n    r = 16, # Choose any number > 0 ! Suggested 8, 16, 32, 64, 128\n    target_modules = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n                      \"gate_proj\", \"up_proj\", \"down_proj\",],\n    lora_alpha = 16,\n    lora_dropout = 0, # Supports any, but = 0 is optimized\n    bias = \"none\",    # Supports any, but = \"none\" is optimized\n    # [NEW] \"unsloth\" uses 30% less VRAM, fits 2x larger batch sizes!\n    use_gradient_checkpointing = \"unsloth\", # True or \"unsloth\" for very long context\n    random_state = 3407,\n    use_rslora = False,  # We support rank stabilized LoRA\n    loftq_config = None, # And LoftQ\n)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"alpaca_prompt = \"\"\"Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n\n### Instruction:\n{}\n\n### Input:\n{}\n\n### Response:\n{}\"\"\"\n\nEOS_TOKEN = tokenizer.eos_token # Must add EOS_TOKEN\ndef formatting_prompts_func(examples):\n    instructions = examples[\"instruction\"]\n    inputs       = examples[\"input\"]\n    outputs      = examples[\"output\"]\n    texts = []\n    for instruction, input, output in zip(instructions, inputs, outputs):\n        # Must add EOS_TOKEN, otherwise your generation will go on forever!\n        text = alpaca_prompt.format(instruction, input, output) + EOS_TOKEN\n        texts.append(text)\n    return { \"text\" : texts, }\npass\n\nfrom datasets import load_dataset\ndataset = load_dataset(\"yahma/alpaca-cleaned\", split = \"train\")\ndataset = dataset.map(formatting_prompts_func, batched = True,)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!pip install mlflow -U --no-cache-dir","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import os\nos.getcwd()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!ls -a","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!mkdir mlruns","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import mlflow\nmlflow.set_tracking_uri(\"mlruns\")\nmlflow.set_experiment(\"unsloth\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from trl import SFTTrainer\nfrom transformers import TrainingArguments\nfrom unsloth import is_bfloat16_supported\n\ntrainer = SFTTrainer(\n    model = model,\n    tokenizer = tokenizer,\n    train_dataset = dataset,\n    dataset_text_field = \"text\",\n    max_seq_length = max_seq_length,\n    dataset_num_proc = 2,\n    packing = False, # Can make training 5x faster for short sequences.\n    args = TrainingArguments(\n        per_device_train_batch_size = 2,\n        gradient_accumulation_steps = 4,\n        warmup_steps = 5,\n        max_steps = 2,\n        learning_rate = 2e-4,\n        fp16 = not is_bfloat16_supported(),\n        bf16 = is_bfloat16_supported(),\n        logging_steps = 1,\n        optim = \"adamw_8bit\",\n        weight_decay = 0.01,\n        lr_scheduler_type = \"linear\",\n        seed = 3407,\n        output_dir = \"outputs\",\n    ),\n)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# # Install MLflow\n# !pip install mlflow --upgrade --no-cache-dir\n\n# Import required libraries\nimport mlflow\nfrom unsloth import FastLanguageModel\nimport torch\nfrom datasets import load_dataset\nfrom transformers import TrainingArguments\nfrom unsloth import is_bfloat16_supported\nfrom trl import SFTTrainer\n\n# # Initialize MLflow\n# mlflow.set_tracking_uri(\"file:///mnt/data/mlruns\")\n# mlflow.set_experiment(\"your_experiment_name\")\n\n# # Set up model configuration\n# max_seq_length = 2048 # Choose any! We auto support RoPE Scaling internally!\n# dtype = None # None for auto detection. Float16 for Tesla T4, V100, Bfloat16 for Ampere+\n# load_in_4bit = True # Use 4bit quantization to reduce memory usage. Can be False.\n\n# # 4bit pre-quantized models\n# fourbit_models = [\n#     \"unsloth/mistral-7b-v0.3-bnb-4bit\",      # New Mistral v3 2x faster!\n#     \"unsloth/mistral-7b-instruct-v0.3-bnb-4bit\",\n#     \"unsloth/llama-3-8b-bnb-4bit\",           # Llama-3 15 trillion tokens model 2x faster!\n#     \"unsloth/llama-3-8b-Instruct-bnb-4bit\",\n#     \"unsloth/llama-3-70b-bnb-4bit\",\n#     \"unsloth/Phi-3-mini-4k-instruct\",        # Phi-3 2x faster!\n#     \"unsloth/Phi-3-medium-4k-instruct\",\n#     \"unsloth/mistral-7b-bnb-4bit\",\n#     \"unsloth/gemma-7b-bnb-4bit\",             # Gemma 2.2x faster!\n# ] # More models at https://huggingface.co/unsloth\n\n# # Load model and tokenizer\n# model, tokenizer = FastLanguageModel.from_pretrained(\n#     model_name=\"meta-llama/Meta-Llama-3-8B\",\n#     max_seq_length=max_seq_length,\n#     dtype=dtype,\n#     load_in_4bit=load_in_4bit,\n#     token=\"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\", # use one if using gated models like meta-llama/Llama-2-7b-hf\n# )\n# model = FastLanguageModel.get_peft_model(\n#     model,\n#     r=16, # Choose any number > 0 ! Suggested 8, 16, 32, 64, 128\n#     target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n#                     \"gate_proj\", \"up_proj\", \"down_proj\"],\n#     lora_alpha=16,\n#     lora_dropout=0, # Supports any, but = 0 is optimized\n#     bias=\"none\",    # Supports any, but = \"none\" is optimized\n#     use_gradient_checkpointing=\"unsloth\", # True or \"unsloth\" for very long context\n#     random_state=3407,\n#     use_rslora=False,  # We support rank stabilized LoRA\n#     loftq_config=None, # And LoftQ\n# )\n\n# Define prompt and EOS token\nalpaca_prompt = \"\"\"Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n\n### Instruction:\n{}\n\n### Input:\n{}\n\n### Response:\n{}\"\"\"\nEOS_TOKEN = tokenizer.eos_token\n\n# Formatting prompts function\ndef formatting_prompts_func(examples):\n    instructions = examples[\"instruction\"]\n    inputs = examples[\"input\"]\n    outputs = examples[\"output\"]\n    texts = []\n    for instruction, input, output in zip(instructions, inputs, outputs):\n        text = alpaca_prompt.format(instruction, input, output) + EOS_TOKEN\n        texts.append(text)\n    return {\"text\": texts}\n\n# Load and format dataset\ndataset = load_dataset(\"yahma/alpaca-cleaned\", split=\"train\")\ndataset = dataset.map(formatting_prompts_func, batched=True)\n\n# Define training arguments\ntraining_args = TrainingArguments(\n    per_device_train_batch_size=2,\n    gradient_accumulation_steps=4,\n    warmup_steps=5,\n    max_steps=5,\n    learning_rate=2e-4,\n    fp16=not is_bfloat16_supported(),\n    bf16=is_bfloat16_supported(),\n    logging_steps=1,\n    optim=\"adamw_8bit\",\n    weight_decay=0.01,\n    lr_scheduler_type=\"linear\",\n    seed=3407,\n    output_dir=\"outputs\",\n)\n\n# Initialize trainer\ntrainer = SFTTrainer(\n    model=model,\n    tokenizer=tokenizer,\n    train_dataset=dataset,\n    dataset_text_field=\"text\",\n    max_seq_length=max_seq_length,\n    dataset_num_proc=2,\n    packing=False, # Can make training 5x faster for short sequences.\n    args=training_args,\n)\n\ntrainer.train()\n\n# Log parameters and metrics\nmlflow.log_params({\n    \"per_device_train_batch_size\": training_args.per_device_train_batch_size,\n    \"gradient_accumulation_steps\": training_args.gradient_accumulation_steps,\n    \"warmup_steps\": training_args.warmup_steps,\n    \"max_steps\": training_args.max_steps,\n    \"learning_rate\": training_args.learning_rate,\n    \"fp16\": training_args.fp16,\n    \"bf16\": training_args.bf16,\n    \"logging_steps\": training_args.logging_steps,\n    \"weight_decay\": training_args.weight_decay,\n    \"lr_scheduler_type\": training_args.lr_scheduler_type,\n    \"seed\": training_args.seed,\n    \"output_dir\": training_args.output_dir,\n})\n\nmlflow.log_metric(\"final_loss\", trainer.state.log_history[-1]['train_loss'])\n\nimport plotly.graph_objects as go\n\n# Assuming trainer.state.log_history contains the loss values\n# Extract loss values and steps (or epochs) into lists\nloss_values = [log['train_loss'] for log in trainer.state.log_history if 'train_loss' in log]\nsteps = list(range(1, len(loss_values) + 1))\n\n# Create an interactive plot\nfig = go.Figure()\n\n# Add the loss plot\nfig.add_trace(go.Scatter(x=steps, y=loss_values, mode='lines+markers', name='Train Loss'))\n\n# Customize the layout\nfig.update_layout(title='Training Loss Over Time',\n                  xaxis_title='Step',\n                  yaxis_title='Loss',\n                  template='plotly_dark')  # Using a dark theme for better visibility\n\n# Show the plot\nfig.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# alpaca_prompt = Copied from above\nFastLanguageModel.for_inference(model) # Enable native 2x faster inference\ninputs = tokenizer(\n[\n    alpaca_prompt.format(\n        \"Continue the fibonnaci sequence till the maximum you can.\", # instruction\n        \"1, 1, 2, 3, 5, 8\", # input\n        \"\", # output - leave this blank for generation!\n    )\n], return_tensors = \"pt\").to(\"cuda\")\n\nfrom transformers import TextStreamer\ntext_streamer = TextStreamer(tokenizer)\n_ = model.generate(**inputs, streamer = text_streamer, max_new_tokens = 128)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"if False:\n    from unsloth import FastLanguageModel\n    model, tokenizer = FastLanguageModel.from_pretrained(\n        model_name = \"lora_model\", # YOUR MODEL YOU USED FOR TRAINING\n        max_seq_length = max_seq_length,\n        dtype = dtype,\n        load_in_4bit = load_in_4bit,\n    )\n    FastLanguageModel.for_inference(model) # Enable native 2x faster inference\n\n# alpaca_prompt = You MUST copy from above!\n\ninputs = tokenizer(\n[\n    alpaca_prompt.format(\n        \"What is a famous tall tower in Paris?\", # instruction\n        \"\", # input\n        \"\", # output - leave this blank for generation!\n    )\n], return_tensors = \"pt\").to(\"cuda\")\n\noutputs = model.generate(**inputs, max_new_tokens = 64, use_cache = True)\ntokenizer.batch_decode(outputs)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!nvidia-smi","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import torch\n\nprint(f\"CUDA available: {torch.cuda.is_available()}\")\nprint(f\"Number of GPUs: {torch.cuda.device_count()}\")\nfor i in range(torch.cuda.device_count()):\n    print(f\"GPU {i}: {torch.cuda.get_device_name(i)}\")\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!echo $CUDA_VISIBLE_DEVICES","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#@title Show current memory stats\ngpu_stats = torch.cuda.get_device_properties(0)\nstart_gpu_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)\nmax_memory = round(gpu_stats.total_memory / 1024 / 1024 / 1024, 3)\nprint(f\"GPU = {gpu_stats.name}. Max memory = {max_memory} GB.\")\nprint(f\"{start_gpu_memory} GB of memory reserved.\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"break","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Unsloth vLLM on SST5","metadata":{}},{"cell_type":"code","source":"%%capture\n!pip install -U triton --no-cache-dir\n\n# Installs Unsloth, Xformers (Flash Attention) and all other packages!\n!pip install \"unsloth[colab-new] @ git+https://github.com/unslothai/unsloth.git\"\n\n!pip install --no-deps \"trl<0.9.0\" peft accelerate bitsandbytes --no-cache-dir\n\n!pip install -U xformers --no-cache-dir\n\n!pip install -U torch torchvision torchaudio --no-cache-dir\n\n!pip install -U seaborn --no-cache-dir\n\n!pip install -U seaborn --no-cache-dir\n\n# Install required packages\n!pip install -U mlflow datasets transformers peft accelerate trl evaluate torch --no-cache-dir\n\nimport torch\nprint(torch.cuda.is_available())\n!nvcc --version\n\n!nvidia-smi\n\n!export OMP_NUM_THREADS=1\n!export MKL_NUM_THREADS=1\n\nimport torch\n\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\ndevice","metadata":{"execution":{"iopub.status.busy":"2024-06-14T03:35:34.897813Z","iopub.execute_input":"2024-06-14T03:35:34.898601Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import os\n\nos.environ['HUGGINGFACE_TOKEN'] = \"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\"\n\nfrom dotenv import load_dotenv\n\nload_dotenv()  # take environment variables from .env.\n\nhuggingface_token = os.getenv('HUGGINGFACE_TOKEN')\n\nhuggingface_token","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import torch\n\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\ndevice","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Install required packages\n# !pip install -U mlflow datasets transformers peft unsloth trl evaluate torch --no-cache-dir\n\n# Import required libraries\nimport mlflow\nfrom unsloth import FastLanguageModel\nimport torch\nfrom datasets import load_dataset\nfrom transformers import TrainingArguments, AutoTokenizer\nfrom unsloth import is_bfloat16_supported\nfrom trl import SFTTrainer\nimport evaluate\nimport numpy as np\nfrom sklearn.metrics import confusion_matrix, classification_report\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Check if CUDA is available and set device accordingly\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n# Initialize MLflow\nmlflow.set_tracking_uri(\"file:///mnt/data/mlruns\")\nmlflow.set_experiment(\"unsloth-train\")\n\n# Set up model configuration\nmax_seq_length = 2048 # Choose any! We auto support RoPE Scaling internally!\ndtype = None # None for auto detection. Float16 for Tesla T4, V100, Bfloat16 for Ampere+\nload_in_4bit = True # Use 4bit quantization to reduce memory usage. Can be False.\n\n# 4bit pre-quantized models\nfourbit_models = [\n    \"unsloth/mistral-7b-v0.3-bnb-4bit\",\n    \"unsloth/mistral-7b-instruct-v0.3-bnb-4bit\",\n    \"unsloth/llama-3-8b-bnb-4bit\",\n    \"unsloth/llama-3-8b-Instruct-bnb-4bit\",\n    \"unsloth/llama-3-70b-bnb-4bit\",\n    \"unsloth/Phi-3-mini-4k-instruct\",\n    \"unsloth/Phi-3-medium-4k-instruct\",\n    \"unsloth/mistral-7b-bnb-4bit\",\n    \"unsloth/gemma-7b-bnb-4bit\",\n]\n\n# Load model and tokenizer\nmodel, tokenizer = FastLanguageModel.from_pretrained(\n    model_name=\"meta-llama/Meta-Llama-3-8B\",\n    max_seq_length=max_seq_length,\n    dtype=dtype,\n    load_in_4bit=load_in_4bit,\n    token=\"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\", # use one if using gated models like meta-llama/Llama-2-7b-hf\n)\nmodel = FastLanguageModel.get_peft_model(\n    model,\n    r=16, # Choose any number > 0 ! Suggested 8, 16, 32, 64, 128\n    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n                    \"gate_proj\", \"up_proj\", \"down_proj\"],\n    lora_alpha=16,\n    lora_dropout=0, # Supports any, but = 0 is optimized\n    bias=\"none\",    # Supports any, but = \"none\" is optimized\n    use_gradient_checkpointing=\"unsloth\", # True or \"unsloth\" for very long context\n    random_state=3407,\n    use_rslora=False,  # We support rank stabilized LoRA\n    loftq_config=None, # And LoftQ\n).to(device)\n\n# Define prompt and EOS token\nalpaca_prompt = \"\"\"Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n\n### Instruction:\n{}\n\n### Input:\n{}\n\n### Response:\n{}\"\"\"\nEOS_TOKEN = tokenizer.eos_token\n\n# Formatting prompts function\ndef formatting_prompts_func(examples):\n    instructions = examples[\"instruction\"]\n    inputs = examples[\"input\"]\n    outputs = examples[\"output\"]\n    texts = []\n    for instruction, input, output in zip(instructions, inputs, outputs):\n        text = alpaca_prompt.format(instruction, input, output) + EOS_TOKEN\n        texts.append(text)\n    return {\"text\": texts}\n\n# Load and format dataset\nsst5 = load_dataset(\"SetFit/sst5\", split=\"train\")  # Adjusted to SST5 dataset\nsst5 = sst5.map(lambda example: {\"label\": 2 if example[\"label\"] > 3 else (0 if example[\"label\"] < 2 else 1)})  # Grouping into 3 labels\n\n# Define training arguments\ntraining_args = TrainingArguments(\n    per_device_train_batch_size=2,\n    gradient_accumulation_steps=4,\n    warmup_steps=5,\n    max_steps=2,\n    learning_rate=2e-4,\n    fp16=torch.cuda.is_available() and not is_bfloat16_supported(),\n    bf16=is_bfloat16_supported(),\n    logging_steps=1,\n    optim=\"adamw_8bit\",\n    weight_decay=0.01,\n    lr_scheduler_type=\"linear\",\n    seed=3407,\n    output_dir=\"outputs\",\n)\n\n# Initialize trainer\ntrainer = SFTTrainer(\n    model=model,\n    tokenizer=tokenizer,\n    train_dataset=sst5,\n    dataset_text_field=\"text\",\n    max_seq_length=max_seq_length,\n    dataset_num_proc=2,\n    packing=False, # Can make training 5x faster for short sequences.\n    args=training_args,\n)\n\ntrainer.train()\n\n# Log parameters and metrics\nmlflow.log_params({\n    \"per_device_train_batch_size\": training_args.per_device_train_batch_size,\n    \"gradient_accumulation_steps\": training_args.gradient_accumulation_steps,\n    \"warmup_steps\": training_args.warmup_steps,\n    \"max_steps\": training_args.max_steps,\n    \"learning_rate\": training_args.learning_rate,\n    \"fp16\": training_args.fp16,\n    \"bf16\": training_args.bf16,\n    \"logging_steps\": training_args.logging_steps,\n    \"weight_decay\": training_args.weight_decay,\n    \"lr_scheduler_type\": training_args.lr_scheduler_type,\n    \"seed\": training_args.seed,\n    \"output_dir\": training_args.output_dir,\n})\n\nmlflow.log_metric(\"final_loss\", trainer.state.log_history[-1]['train_loss'])\n","metadata":{"execution":{"iopub.status.idle":"2024-06-14T03:42:36.417001Z","shell.execute_reply.started":"2024-06-14T03:40:30.047053Z","shell.execute_reply":"2024-06-14T03:42:36.416020Z"},"trusted":true},"execution_count":4,"outputs":[{"name":"stderr","text":"2024/06/14 03:40:47 INFO mlflow.tracking.fluent: Experiment with name 'unsloth-train' does not exist. Creating a new experiment.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/1.20k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3ccba7bd84a5454d8100afc1d3da8094"}},"metadata":{}},{"name":"stdout","text":"==((====))==  Unsloth: Fast Llama patching release 2024.6\n   \\\\   /|    GPU: Tesla T4. Max memory: 14.748 GB. Platform = Linux.\nO^O/ \\_/ \\    Pytorch: 2.3.1+cu121. CUDA = 7.5. CUDA Toolkit = 12.1.\n\\        /    Bfloat16 = FALSE. Xformers = 0.0.26.post1. FA = False.\n \"-____-\"     Free Apache license: http://github.com/unslothai/unsloth\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/5.70G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2a733ef30e82483ea8d05028ae06b488"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/172 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f42f53f593664178a83309b2ffe17eba"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/50.6k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"686e6906f4c340519f1637227377fe86"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/9.09M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f967a89d3206410fb36ed0c0b3e5a624"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/464 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3aa6f383917244d7a573fddd9838fb90"}},"metadata":{}},{"name":"stderr","text":"Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\nUnsloth 2024.6 patched 32 layers with 32 QKV layers, 32 O layers and 32 MLP layers.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Downloading readme:   0%|          | 0.00/421 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7bb2da627ab54fb289f496fceb24f2eb"}},"metadata":{}},{"name":"stderr","text":"Repo card metadata block was not found. Setting CardData to empty.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Downloading data:   0%|          | 0.00/1.32M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"55be20a878e84c88a31fb7c9d064ba37"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading data:   0%|          | 0.00/171k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"14d52e9796a24c1fb9491ee919a0bae4"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading data:   0%|          | 0.00/343k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6b49125f8fbb4a3795e68435a1f197ba"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating train split:   0%|          | 0/8544 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"710a6e3c05404979a2fbeb53727fc8ea"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating validation split:   0%|          | 0/1101 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6769aef71dfb4cbeac5d1c1388087b3e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating test split:   0%|          | 0/2210 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4e5f56189bc14b2985c3aa232f7380d2"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/8544 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0b67f6f547814d93b7e6e4d487a6c9f4"}},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/transformers/training_args.py:1965: FutureWarning: `--push_to_hub_token` is deprecated and will be removed in version 5 of 🤗 Transformers. Use `--hub_token` instead.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/trl/trainer/sft_trainer.py:269: UserWarning: You passed a `max_seq_length` argument to the SFTTrainer, the value you passed will override the one in the `SFTConfig`.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/trl/trainer/sft_trainer.py:283: UserWarning: You passed a `dataset_num_proc` argument to the SFTTrainer, the value you passed will override the one in the `SFTConfig`.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/trl/trainer/sft_trainer.py:307: UserWarning: You passed a `dataset_text_field` argument to the SFTTrainer, the value you passed will override the one in the `SFTConfig`.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/multiprocess/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n  self.pid = os.fork()\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Map (num_proc=2):   0%|          | 0/8544 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"97fbffe45fcd44938a35b1f4962827b6"}},"metadata":{}},{"name":"stderr","text":"max_steps is given, it will override any value given in num_train_epochs\n==((====))==  Unsloth - 2x faster free finetuning | Num GPUs = 1\n   \\\\   /|    Num examples = 8,544 | Num Epochs = 1\nO^O/ \\_/ \\    Batch size per device = 2 | Gradient Accumulation steps = 4\n\\        /    Total batch size = 8 | Total steps = 2\n \"-____-\"     Number of trainable parameters = 41,943,040\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m The `run_name` is currently set to the same value as `TrainingArguments.output_dir`. If this was not intended, please specify a different run name by setting the `TrainingArguments.run_name` parameter.\n\u001b[34m\u001b[1mwandb\u001b[0m: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize\n\u001b[34m\u001b[1mwandb\u001b[0m: Paste an API key from your profile and hit enter, or press ctrl+c to quit:","output_type":"stream"},{"output_type":"stream","name":"stdin","text":"  ········································\n"},{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"wandb version 0.17.1 is available!  To upgrade, please run:\n $ pip install wandb --upgrade"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Tracking run with wandb version 0.17.0"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Run data is saved locally in <code>/kaggle/working/wandb/run-20240614_034208-ut9gmek6</code>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Syncing run <strong><a href='https://wandb.ai/mdrafatsiddiqui/huggingface/runs/ut9gmek6' target=\"_blank\">outputs</a></strong> to <a href='https://wandb.ai/mdrafatsiddiqui/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View project at <a href='https://wandb.ai/mdrafatsiddiqui/huggingface' target=\"_blank\">https://wandb.ai/mdrafatsiddiqui/huggingface</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run at <a href='https://wandb.ai/mdrafatsiddiqui/huggingface/runs/ut9gmek6' target=\"_blank\">https://wandb.ai/mdrafatsiddiqui/huggingface/runs/ut9gmek6</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='2' max='2' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [2/2 00:02, Epoch 0/1]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>4.384500</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>4.608300</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}}]},{"cell_type":"code","source":"!nvidia-smi","metadata":{"execution":{"iopub.status.busy":"2024-06-14T03:42:36.420945Z","iopub.execute_input":"2024-06-14T03:42:36.421243Z","iopub.status.idle":"2024-06-14T03:42:37.505342Z","shell.execute_reply.started":"2024-06-14T03:42:36.421215Z","shell.execute_reply":"2024-06-14T03:42:37.504150Z"},"trusted":true},"execution_count":5,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/pty.py:89: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n  pid, fd = os.forkpty()\n","output_type":"stream"},{"name":"stdout","text":"Fri Jun 14 03:42:37 2024       \n+---------------------------------------------------------------------------------------+\n| NVIDIA-SMI 535.129.03             Driver Version: 535.129.03   CUDA Version: 12.2     |\n|-----------------------------------------+----------------------+----------------------+\n| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n|                                         |                      |               MIG M. |\n|=========================================+======================+======================|\n|   0  Tesla T4                       Off | 00000000:00:04.0 Off |                    0 |\n| N/A   54C    P0              36W /  70W |   6301MiB / 15360MiB |      0%      Default |\n|                                         |                      |                  N/A |\n+-----------------------------------------+----------------------+----------------------+\n|   1  Tesla T4                       Off | 00000000:00:05.0 Off |                    0 |\n| N/A   39C    P8               9W /  70W |      3MiB / 15360MiB |      0%      Default |\n|                                         |                      |                  N/A |\n+-----------------------------------------+----------------------+----------------------+\n                                                                                         \n+---------------------------------------------------------------------------------------+\n| Processes:                                                                            |\n|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n|        ID   ID                                                             Usage      |\n|=======================================================================================|\n+---------------------------------------------------------------------------------------+\n","output_type":"stream"}]},{"cell_type":"code","source":"del trainer\ndel sst5\ntorch.cuda.empty_cache()\n\n!nvidia-smi","metadata":{"execution":{"iopub.status.busy":"2024-06-14T03:42:37.507175Z","iopub.execute_input":"2024-06-14T03:42:37.507599Z","iopub.status.idle":"2024-06-14T03:42:38.593499Z","shell.execute_reply.started":"2024-06-14T03:42:37.507558Z","shell.execute_reply":"2024-06-14T03:42:38.592300Z"},"trusted":true},"execution_count":6,"outputs":[{"name":"stdout","text":"Fri Jun 14 03:42:38 2024       \n+---------------------------------------------------------------------------------------+\n| NVIDIA-SMI 535.129.03             Driver Version: 535.129.03   CUDA Version: 12.2     |\n|-----------------------------------------+----------------------+----------------------+\n| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n|                                         |                      |               MIG M. |\n|=========================================+======================+======================|\n|   0  Tesla T4                       Off | 00000000:00:04.0 Off |                    0 |\n| N/A   54C    P0              29W /  70W |   6119MiB / 15360MiB |      0%      Default |\n|                                         |                      |                  N/A |\n+-----------------------------------------+----------------------+----------------------+\n|   1  Tesla T4                       Off | 00000000:00:05.0 Off |                    0 |\n| N/A   39C    P8               9W /  70W |      3MiB / 15360MiB |      0%      Default |\n|                                         |                      |                  N/A |\n+-----------------------------------------+----------------------+----------------------+\n                                                                                         \n+---------------------------------------------------------------------------------------+\n| Processes:                                                                            |\n|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n|        ID   ID                                                             Usage      |\n|=======================================================================================|\n+---------------------------------------------------------------------------------------+\n","output_type":"stream"}]},{"cell_type":"code","source":"import accelerate\nfrom datasets import load_dataset\nfrom transformers import AutoTokenizer\nimport torch\nimport numpy as np\nfrom sklearn.metrics import confusion_matrix, classification_report\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Initialize the accelerator\naccelerator = accelerate.Accelerator()\n\n# Function to get predictions from vllm using batch processing\ndef get_vllm_predictions(model, tokenizer, dataset, batch_size=32):\n    model.eval()\n    predictions, labels = [], []\n    for i in range(0, len(dataset), batch_size):\n        batch = dataset[i:i+batch_size]\n        inputs = tokenizer(batch[\"text\"], return_tensors=\"pt\", max_length=max_seq_length, truncation=True, padding=True).to(device)\n        with torch.no_grad():\n            outputs = model(**inputs)\n        logits = outputs.logits\n        preds = torch.argmax(logits, dim=-1).cpu().numpy()\n        predictions.extend(preds)\n        labels.extend(batch[\"label\"])\n    return predictions, labels\n\n# Load tokenizer and model (assuming they are defined earlier in the script)\n# tokenizer = AutoTokenizer.from_pretrained(\"model_name\")\n# model = SomeModelClass.from_pretrained(\"model_name\")\n\n# Load validation and test sets and group into 3 labels\nval_set = load_dataset(\"SetFit/sst5\", split=\"validation\").map(lambda example: {\"label\": 2 if example[\"label\"] > 3 else (0 if example[\"label\"] < 2 else 1)})\ntest_set = load_dataset(\"SetFit/sst5\", split=\"test\").map(lambda example: {\"label\": 2 if example[\"label\"] > 3 else (0 if example[\"label\"] < 2 else 1)})\n\n# Ensure labels are integers\nval_set = val_set.map(lambda example: {\"label\": int(example[\"label\"])})\ntest_set = test_set.map(lambda example: {\"label\": int(example[\"label\"])})\n\n# Move model to the correct device\nmodel = accelerator.prepare(model)\n\n# Get predictions\nval_predictions, val_labels = get_vllm_predictions(model, tokenizer, val_set)\ntest_predictions, test_labels = get_vllm_predictions(model, tokenizer, test_set)\n\n# Ensure predictions and labels are numpy arrays of integer type\nval_predictions = np.array(val_predictions, dtype=int)\nval_labels = np.array(val_labels, dtype=int)\ntest_predictions = np.array(test_predictions, dtype=int)\ntest_labels = np.array(test_labels, dtype=int)\n\n# Check for consistency in the data types and lengths\nprint(\"Validation Labels Type:\", val_labels.dtype, \"Length:\", len(val_labels))\nprint(\"Validation Predictions Type:\", val_predictions.dtype, \"Length:\", len(val_predictions))\nprint(\"Test Labels Type:\", test_labels.dtype, \"Length:\", len(test_labels))\nprint(\"Test Predictions Type:\", test_predictions.dtype, \"Length:\", len(test_predictions))\n\n# Calculate confusion matrix and classification report\nval_conf_matrix = confusion_matrix(val_labels, val_predictions)\ntest_conf_matrix = confusion_matrix(test_labels, test_predictions)\n\n# Plot confusion matrix for validation set\nplt.figure(figsize=(10, 7))\nsns.heatmap(val_conf_matrix, annot=True, fmt='d', cmap='Blues', xticklabels=[\"Negative\", \"Neutral\", \"Positive\"], yticklabels=[\"Negative\", \"Neutral\", \"Positive\"])\nplt.xlabel('Predicted')\nplt.ylabel('Actual')\nplt.title('Validation Set Confusion Matrix')\nplt.show()\n\n# Plot confusion matrix for test set\nplt.figure(figsize=(10, 7))\nsns.heatmap(test_conf_matrix, annot=True, fmt='d', cmap='Blues', xticklabels=[\"Negative\", \"Neutral\", \"Positive\"], yticklabels=[\"Negative\", \"Neutral\", \"Positive\"])\nplt.xlabel('Predicted')\nplt.ylabel('Actual')\nplt.title('Test Set Confusion Matrix')\nplt.show()\n\n# Print classification reports\nprint(\"Validation Set Classification Report:\")\nprint(classification_report(val_labels, val_predictions, target_names=[\"Negative\", \"Neutral\", \"Positive\"]))\n\nprint(\"Test Set Classification Report:\")\nprint(classification_report(test_labels, test_predictions, target_names=[\"Negative\", \"Neutral\", \"Positive\"]))\n","metadata":{"execution":{"iopub.status.busy":"2024-06-14T03:42:38.595294Z","iopub.execute_input":"2024-06-14T03:42:38.595611Z","iopub.status.idle":"2024-06-14T03:45:08.933107Z","shell.execute_reply.started":"2024-06-14T03:42:38.595580Z","shell.execute_reply":"2024-06-14T03:45:08.931367Z"},"trusted":true},"execution_count":7,"outputs":[{"name":"stderr","text":"Repo card metadata block was not found. Setting CardData to empty.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/1101 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"002943389b604176a2f7c9c23ef67d09"}},"metadata":{}},{"name":"stderr","text":"Repo card metadata block was not found. Setting CardData to empty.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/2210 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7443e1955add47eda722f669624b888a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/1101 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8db4eb3ae4804b5c8b585ebf509d4895"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/2210 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6a2ef9b18fa64f1ea41054482d03d070"}},"metadata":{}},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)","Cell \u001b[0;32mIn[7], line 48\u001b[0m\n\u001b[1;32m     45\u001b[0m test_predictions, test_labels \u001b[38;5;241m=\u001b[39m get_vllm_predictions(model, tokenizer, test_set)\n\u001b[1;32m     47\u001b[0m \u001b[38;5;66;03m# Ensure predictions and labels are numpy arrays of integer type\u001b[39;00m\n\u001b[0;32m---> 48\u001b[0m val_predictions \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43marray\u001b[49m\u001b[43m(\u001b[49m\u001b[43mval_predictions\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mint\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     49\u001b[0m val_labels \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray(val_labels, dtype\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mint\u001b[39m)\n\u001b[1;32m     50\u001b[0m test_predictions \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray(test_predictions, dtype\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mint\u001b[39m)\n","\u001b[0;31mValueError\u001b[0m: setting an array element with a sequence. The requested array has an inhomogeneous shape after 1 dimensions. The detected shape was (1101,) + inhomogeneous part."],"ename":"ValueError","evalue":"setting an array element with a sequence. The requested array has an inhomogeneous shape after 1 dimensions. The detected shape was (1101,) + inhomogeneous part.","output_type":"error"}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"val_predictions = np.array(val_predictions, dtype=int)","metadata":{"execution":{"iopub.status.busy":"2024-06-14T03:45:08.934456Z","iopub.status.idle":"2024-06-14T03:45:08.935173Z","shell.execute_reply.started":"2024-06-14T03:45:08.934979Z","shell.execute_reply":"2024-06-14T03:45:08.934997Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"val_predictions","metadata":{"execution":{"iopub.status.busy":"2024-06-14T03:27:23.351172Z","iopub.status.idle":"2024-06-14T03:27:23.351478Z","shell.execute_reply.started":"2024-06-14T03:27:23.351324Z","shell.execute_reply":"2024-06-14T03:27:23.351336Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}