{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":30732,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install -U nbformat --no-cache-dir","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!pip install -U triton --no-cache-dir","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!pip install -U accelerate --no-cache-dir","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%capture\n# Installs Unsloth, Xformers (Flash Attention) and all other packages!\n!pip install \"unsloth[colab-new] @ git+https://github.com/unslothai/unsloth.git\"\n!pip install --no-deps \"trl<0.9.0\" peft accelerate bitsandbytes --no-cache-dir","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!pip install -U xformers --no-cache-dir","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!pip install -U torch torchvision torchaudio --no-cache-dir","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import torch\nprint(torch.cuda.is_available())","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!nvcc --version","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!export OMP_NUM_THREADS=1\n!export MKL_NUM_THREADS=1","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from unsloth import FastLanguageModel\nimport torch\nmax_seq_length = 2048 # Choose any! We auto support RoPE Scaling internally!\ndtype = None # None for auto detection. Float16 for Tesla T4, V100, Bfloat16 for Ampere+\nload_in_4bit = True # Use 4bit quantization to reduce memory usage. Can be False.\n\n# 4bit pre quantized models we support for 4x faster downloading + no OOMs.\nfourbit_models = [\n    \"unsloth/mistral-7b-v0.3-bnb-4bit\",      # New Mistral v3 2x faster!\n    \"unsloth/mistral-7b-instruct-v0.3-bnb-4bit\",\n    \"unsloth/llama-3-8b-bnb-4bit\",           # Llama-3 15 trillion tokens model 2x faster!\n    \"unsloth/llama-3-8b-Instruct-bnb-4bit\",\n    \"unsloth/llama-3-70b-bnb-4bit\",\n    \"unsloth/Phi-3-mini-4k-instruct\",        # Phi-3 2x faster!\n    \"unsloth/Phi-3-medium-4k-instruct\",\n    \"unsloth/mistral-7b-bnb-4bit\",\n    \"unsloth/gemma-7b-bnb-4bit\",             # Gemma 2.2x faster!\n] # More models at https://huggingface.co/unsloth\n\nmodel, tokenizer = FastLanguageModel.from_pretrained(\n    model_name = \"meta-llama/Meta-Llama-3-8B\",\n    max_seq_length = max_seq_length,\n    dtype = dtype,\n    load_in_4bit = load_in_4bit,\n    token = \"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\", # use one if using gated models like meta-llama/Llama-2-7b-hf\n)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model = FastLanguageModel.get_peft_model(\n    model,\n    r = 16, # Choose any number > 0 ! Suggested 8, 16, 32, 64, 128\n    target_modules = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n                      \"gate_proj\", \"up_proj\", \"down_proj\",],\n    lora_alpha = 16,\n    lora_dropout = 0, # Supports any, but = 0 is optimized\n    bias = \"none\",    # Supports any, but = \"none\" is optimized\n    # [NEW] \"unsloth\" uses 30% less VRAM, fits 2x larger batch sizes!\n    use_gradient_checkpointing = \"unsloth\", # True or \"unsloth\" for very long context\n    random_state = 3407,\n    use_rslora = False,  # We support rank stabilized LoRA\n    loftq_config = None, # And LoftQ\n)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"alpaca_prompt = \"\"\"Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n\n### Instruction:\n{}\n\n### Input:\n{}\n\n### Response:\n{}\"\"\"\n\nEOS_TOKEN = tokenizer.eos_token # Must add EOS_TOKEN\ndef formatting_prompts_func(examples):\n    instructions = examples[\"instruction\"]\n    inputs       = examples[\"input\"]\n    outputs      = examples[\"output\"]\n    texts = []\n    for instruction, input, output in zip(instructions, inputs, outputs):\n        # Must add EOS_TOKEN, otherwise your generation will go on forever!\n        text = alpaca_prompt.format(instruction, input, output) + EOS_TOKEN\n        texts.append(text)\n    return { \"text\" : texts, }\npass\n\nfrom datasets import load_dataset\ndataset = load_dataset(\"yahma/alpaca-cleaned\", split = \"train\")\ndataset = dataset.map(formatting_prompts_func, batched = True,)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!pip install mlflow -U --no-cache-dir","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import os\nos.getcwd()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!ls -a","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!mkdir mlruns","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import mlflow\nmlflow.set_tracking_uri(\"mlruns\")\nmlflow.set_experiment(\"unsloth\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from trl import SFTTrainer\nfrom transformers import TrainingArguments\nfrom unsloth import is_bfloat16_supported\n\ntrainer = SFTTrainer(\n    model = model,\n    tokenizer = tokenizer,\n    train_dataset = dataset,\n    dataset_text_field = \"text\",\n    max_seq_length = max_seq_length,\n    dataset_num_proc = 2,\n    packing = False, # Can make training 5x faster for short sequences.\n    args = TrainingArguments(\n        per_device_train_batch_size = 2,\n        gradient_accumulation_steps = 4,\n        warmup_steps = 5,\n        max_steps = 2,\n        learning_rate = 2e-4,\n        fp16 = not is_bfloat16_supported(),\n        bf16 = is_bfloat16_supported(),\n        logging_steps = 1,\n        optim = \"adamw_8bit\",\n        weight_decay = 0.01,\n        lr_scheduler_type = \"linear\",\n        seed = 3407,\n        output_dir = \"outputs\",\n    ),\n)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# # Install MLflow\n# !pip install mlflow --upgrade --no-cache-dir\n\n# Import required libraries\nimport mlflow\nfrom unsloth import FastLanguageModel\nimport torch\nfrom datasets import load_dataset\nfrom transformers import TrainingArguments\nfrom unsloth import is_bfloat16_supported\nfrom trl import SFTTrainer\n\n# # Initialize MLflow\n# mlflow.set_tracking_uri(\"file:///mnt/data/mlruns\")\n# mlflow.set_experiment(\"your_experiment_name\")\n\n# # Set up model configuration\n# max_seq_length = 2048 # Choose any! We auto support RoPE Scaling internally!\n# dtype = None # None for auto detection. Float16 for Tesla T4, V100, Bfloat16 for Ampere+\n# load_in_4bit = True # Use 4bit quantization to reduce memory usage. Can be False.\n\n# # 4bit pre-quantized models\n# fourbit_models = [\n#     \"unsloth/mistral-7b-v0.3-bnb-4bit\",      # New Mistral v3 2x faster!\n#     \"unsloth/mistral-7b-instruct-v0.3-bnb-4bit\",\n#     \"unsloth/llama-3-8b-bnb-4bit\",           # Llama-3 15 trillion tokens model 2x faster!\n#     \"unsloth/llama-3-8b-Instruct-bnb-4bit\",\n#     \"unsloth/llama-3-70b-bnb-4bit\",\n#     \"unsloth/Phi-3-mini-4k-instruct\",        # Phi-3 2x faster!\n#     \"unsloth/Phi-3-medium-4k-instruct\",\n#     \"unsloth/mistral-7b-bnb-4bit\",\n#     \"unsloth/gemma-7b-bnb-4bit\",             # Gemma 2.2x faster!\n# ] # More models at https://huggingface.co/unsloth\n\n# # Load model and tokenizer\n# model, tokenizer = FastLanguageModel.from_pretrained(\n#     model_name=\"meta-llama/Meta-Llama-3-8B\",\n#     max_seq_length=max_seq_length,\n#     dtype=dtype,\n#     load_in_4bit=load_in_4bit,\n#     token=\"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\", # use one if using gated models like meta-llama/Llama-2-7b-hf\n# )\n# model = FastLanguageModel.get_peft_model(\n#     model,\n#     r=16, # Choose any number > 0 ! Suggested 8, 16, 32, 64, 128\n#     target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n#                     \"gate_proj\", \"up_proj\", \"down_proj\"],\n#     lora_alpha=16,\n#     lora_dropout=0, # Supports any, but = 0 is optimized\n#     bias=\"none\",    # Supports any, but = \"none\" is optimized\n#     use_gradient_checkpointing=\"unsloth\", # True or \"unsloth\" for very long context\n#     random_state=3407,\n#     use_rslora=False,  # We support rank stabilized LoRA\n#     loftq_config=None, # And LoftQ\n# )\n\n# Define prompt and EOS token\nalpaca_prompt = \"\"\"Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n\n### Instruction:\n{}\n\n### Input:\n{}\n\n### Response:\n{}\"\"\"\nEOS_TOKEN = tokenizer.eos_token\n\n# Formatting prompts function\ndef formatting_prompts_func(examples):\n    instructions = examples[\"instruction\"]\n    inputs = examples[\"input\"]\n    outputs = examples[\"output\"]\n    texts = []\n    for instruction, input, output in zip(instructions, inputs, outputs):\n        text = alpaca_prompt.format(instruction, input, output) + EOS_TOKEN\n        texts.append(text)\n    return {\"text\": texts}\n\n# Load and format dataset\ndataset = load_dataset(\"yahma/alpaca-cleaned\", split=\"train\")\ndataset = dataset.map(formatting_prompts_func, batched=True)\n\n# Define training arguments\ntraining_args = TrainingArguments(\n    per_device_train_batch_size=2,\n    gradient_accumulation_steps=4,\n    warmup_steps=5,\n    max_steps=5,\n    learning_rate=2e-4,\n    fp16=not is_bfloat16_supported(),\n    bf16=is_bfloat16_supported(),\n    logging_steps=1,\n    optim=\"adamw_8bit\",\n    weight_decay=0.01,\n    lr_scheduler_type=\"linear\",\n    seed=3407,\n    output_dir=\"outputs\",\n)\n\n# Initialize trainer\ntrainer = SFTTrainer(\n    model=model,\n    tokenizer=tokenizer,\n    train_dataset=dataset,\n    dataset_text_field=\"text\",\n    max_seq_length=max_seq_length,\n    dataset_num_proc=2,\n    packing=False, # Can make training 5x faster for short sequences.\n    args=training_args,\n)\n\ntrainer.train()\n\n# Log parameters and metrics\nmlflow.log_params({\n    \"per_device_train_batch_size\": training_args.per_device_train_batch_size,\n    \"gradient_accumulation_steps\": training_args.gradient_accumulation_steps,\n    \"warmup_steps\": training_args.warmup_steps,\n    \"max_steps\": training_args.max_steps,\n    \"learning_rate\": training_args.learning_rate,\n    \"fp16\": training_args.fp16,\n    \"bf16\": training_args.bf16,\n    \"logging_steps\": training_args.logging_steps,\n    \"weight_decay\": training_args.weight_decay,\n    \"lr_scheduler_type\": training_args.lr_scheduler_type,\n    \"seed\": training_args.seed,\n    \"output_dir\": training_args.output_dir,\n})\n\nmlflow.log_metric(\"final_loss\", trainer.state.log_history[-1]['train_loss'])\n\nimport plotly.graph_objects as go\n\n# Assuming trainer.state.log_history contains the loss values\n# Extract loss values and steps (or epochs) into lists\nloss_values = [log['train_loss'] for log in trainer.state.log_history if 'train_loss' in log]\nsteps = list(range(1, len(loss_values) + 1))\n\n# Create an interactive plot\nfig = go.Figure()\n\n# Add the loss plot\nfig.add_trace(go.Scatter(x=steps, y=loss_values, mode='lines+markers', name='Train Loss'))\n\n# Customize the layout\nfig.update_layout(title='Training Loss Over Time',\n                  xaxis_title='Step',\n                  yaxis_title='Loss',\n                  template='plotly_dark')  # Using a dark theme for better visibility\n\n# Show the plot\nfig.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# alpaca_prompt = Copied from above\nFastLanguageModel.for_inference(model) # Enable native 2x faster inference\ninputs = tokenizer(\n[\n    alpaca_prompt.format(\n        \"Continue the fibonnaci sequence till the maximum you can.\", # instruction\n        \"1, 1, 2, 3, 5, 8\", # input\n        \"\", # output - leave this blank for generation!\n    )\n], return_tensors = \"pt\").to(\"cuda\")\n\nfrom transformers import TextStreamer\ntext_streamer = TextStreamer(tokenizer)\n_ = model.generate(**inputs, streamer = text_streamer, max_new_tokens = 128)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"if False:\n    from unsloth import FastLanguageModel\n    model, tokenizer = FastLanguageModel.from_pretrained(\n        model_name = \"lora_model\", # YOUR MODEL YOU USED FOR TRAINING\n        max_seq_length = max_seq_length,\n        dtype = dtype,\n        load_in_4bit = load_in_4bit,\n    )\n    FastLanguageModel.for_inference(model) # Enable native 2x faster inference\n\n# alpaca_prompt = You MUST copy from above!\n\ninputs = tokenizer(\n[\n    alpaca_prompt.format(\n        \"What is a famous tall tower in Paris?\", # instruction\n        \"\", # input\n        \"\", # output - leave this blank for generation!\n    )\n], return_tensors = \"pt\").to(\"cuda\")\n\noutputs = model.generate(**inputs, max_new_tokens = 64, use_cache = True)\ntokenizer.batch_decode(outputs)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!nvidia-smi","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import torch\n\nprint(f\"CUDA available: {torch.cuda.is_available()}\")\nprint(f\"Number of GPUs: {torch.cuda.device_count()}\")\nfor i in range(torch.cuda.device_count()):\n    print(f\"GPU {i}: {torch.cuda.get_device_name(i)}\")\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!echo $CUDA_VISIBLE_DEVICES","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#@title Show current memory stats\ngpu_stats = torch.cuda.get_device_properties(0)\nstart_gpu_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)\nmax_memory = round(gpu_stats.total_memory / 1024 / 1024 / 1024, 3)\nprint(f\"GPU = {gpu_stats.name}. Max memory = {max_memory} GB.\")\nprint(f\"{start_gpu_memory} GB of memory reserved.\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"break","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Unsloth vLLM on SST5","metadata":{}},{"cell_type":"code","source":"%%capture\n!pip install -U triton --no-cache-dir\n\n# Installs Unsloth, Xformers (Flash Attention) and all other packages!\n!pip install \"unsloth[colab-new] @ git+https://github.com/unslothai/unsloth.git\"\n\n!pip install --no-deps \"trl<0.9.0\" peft accelerate bitsandbytes --no-cache-dir\n\n!pip install -U xformers --no-cache-dir\n\n!pip install -U torch torchvision torchaudio --no-cache-dir\n\n!pip install -U seaborn --no-cache-dir\n\n!pip install -U seaborn --no-cache-dir\n\n# Install required packages\n!pip install mlflow datasets transformers peft accelerate trl evaluate torch --upgrade --no-cache-dir\n\nimport torch\nprint(torch.cuda.is_available())\n!nvcc --version\n\n!nvidia-smi\n\n!export OMP_NUM_THREADS=1\n!export MKL_NUM_THREADS=1\n\nimport torch\n\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\ndevice","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import os\n\nos.environ['HUGGINGFACE_TOKEN'] = \"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\"\n\nfrom dotenv import load_dotenv\n\nload_dotenv()  # take environment variables from .env.\n\nhuggingface_token = os.getenv('HUGGINGFACE_TOKEN')\n\nhuggingface_token","metadata":{"execution":{"iopub.status.busy":"2024-06-14T03:26:02.548689Z","iopub.execute_input":"2024-06-14T03:26:02.549070Z","iopub.status.idle":"2024-06-14T03:26:02.581602Z","shell.execute_reply.started":"2024-06-14T03:26:02.549039Z","shell.execute_reply":"2024-06-14T03:26:02.580636Z"},"trusted":true},"execution_count":1,"outputs":[{"execution_count":1,"output_type":"execute_result","data":{"text/plain":"'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'"},"metadata":{}}]},{"cell_type":"code","source":"import torch\n\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\ndevice","metadata":{"execution":{"iopub.status.busy":"2024-06-14T03:26:02.583403Z","iopub.execute_input":"2024-06-14T03:26:02.583729Z","iopub.status.idle":"2024-06-14T03:26:07.188370Z","shell.execute_reply.started":"2024-06-14T03:26:02.583705Z","shell.execute_reply":"2024-06-14T03:26:07.186695Z"},"trusted":true},"execution_count":2,"outputs":[{"execution_count":2,"output_type":"execute_result","data":{"text/plain":"device(type='cuda')"},"metadata":{}}]},{"cell_type":"code","source":"# Install required packages\n!pip install mlflow datasets transformers peft unsloth trl evaluate torch --upgrade --no-cache-dir\n\n# Import required libraries\nimport mlflow\nfrom unsloth import FastLanguageModel\nimport torch\nfrom datasets import load_dataset\nfrom transformers import TrainingArguments, AutoTokenizer\nfrom unsloth import is_bfloat16_supported\nfrom trl import SFTTrainer\nimport evaluate\nimport numpy as np\nfrom sklearn.metrics import confusion_matrix, classification_report\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Check if CUDA is available and set device accordingly\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n# Initialize MLflow\nmlflow.set_tracking_uri(\"file:///mnt/data/mlruns\")\nmlflow.set_experiment(\"unsloth-train\")\n\n# Set up model configuration\nmax_seq_length = 2048 # Choose any! We auto support RoPE Scaling internally!\ndtype = None # None for auto detection. Float16 for Tesla T4, V100, Bfloat16 for Ampere+\nload_in_4bit = True # Use 4bit quantization to reduce memory usage. Can be False.\n\n# 4bit pre-quantized models\nfourbit_models = [\n    \"unsloth/mistral-7b-v0.3-bnb-4bit\",\n    \"unsloth/mistral-7b-instruct-v0.3-bnb-4bit\",\n    \"unsloth/llama-3-8b-bnb-4bit\",\n    \"unsloth/llama-3-8b-Instruct-bnb-4bit\",\n    \"unsloth/llama-3-70b-bnb-4bit\",\n    \"unsloth/Phi-3-mini-4k-instruct\",\n    \"unsloth/Phi-3-medium-4k-instruct\",\n    \"unsloth/mistral-7b-bnb-4bit\",\n    \"unsloth/gemma-7b-bnb-4bit\",\n]\n\n# Load model and tokenizer\nmodel, tokenizer = FastLanguageModel.from_pretrained(\n    model_name=\"meta-llama/Meta-Llama-3-8B\",\n    max_seq_length=max_seq_length,\n    dtype=dtype,\n    load_in_4bit=load_in_4bit,\n    token=\"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\", # use one if using gated models like meta-llama/Llama-2-7b-hf\n)\nmodel = FastLanguageModel.get_peft_model(\n    model,\n    r=16, # Choose any number > 0 ! Suggested 8, 16, 32, 64, 128\n    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n                    \"gate_proj\", \"up_proj\", \"down_proj\"],\n    lora_alpha=16,\n    lora_dropout=0, # Supports any, but = 0 is optimized\n    bias=\"none\",    # Supports any, but = \"none\" is optimized\n    use_gradient_checkpointing=\"unsloth\", # True or \"unsloth\" for very long context\n    random_state=3407,\n    use_rslora=False,  # We support rank stabilized LoRA\n    loftq_config=None, # And LoftQ\n).to(device)\n\n# Define prompt and EOS token\nalpaca_prompt = \"\"\"Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n\n### Instruction:\n{}\n\n### Input:\n{}\n\n### Response:\n{}\"\"\"\nEOS_TOKEN = tokenizer.eos_token\n\n# Formatting prompts function\ndef formatting_prompts_func(examples):\n    instructions = examples[\"instruction\"]\n    inputs = examples[\"input\"]\n    outputs = examples[\"output\"]\n    texts = []\n    for instruction, input, output in zip(instructions, inputs, outputs):\n        text = alpaca_prompt.format(instruction, input, output) + EOS_TOKEN\n        texts.append(text)\n    return {\"text\": texts}\n\n# Load and format dataset\nsst5 = load_dataset(\"SetFit/sst5\", split=\"train\")  # Adjusted to SST5 dataset\nsst5 = sst5.map(lambda example: {\"label\": 2 if example[\"label\"] > 3 else (0 if example[\"label\"] < 2 else 1)})  # Grouping into 3 labels\n\n# Define training arguments\ntraining_args = TrainingArguments(\n    per_device_train_batch_size=2,\n    gradient_accumulation_steps=4,\n    warmup_steps=5,\n    max_steps=2,\n    learning_rate=2e-4,\n    fp16=torch.cuda.is_available() and not is_bfloat16_supported(),\n    bf16=is_bfloat16_supported(),\n    logging_steps=1,\n    optim=\"adamw_8bit\",\n    weight_decay=0.01,\n    lr_scheduler_type=\"linear\",\n    seed=3407,\n    output_dir=\"outputs\",\n)\n\n# Initialize trainer\ntrainer = SFTTrainer(\n    model=model,\n    tokenizer=tokenizer,\n    train_dataset=sst5,\n    dataset_text_field=\"text\",\n    max_seq_length=max_seq_length,\n    dataset_num_proc=2,\n    packing=False, # Can make training 5x faster for short sequences.\n    args=training_args,\n)\n\ntrainer.train()\n\n# Log parameters and metrics\nmlflow.log_params({\n    \"per_device_train_batch_size\": training_args.per_device_train_batch_size,\n    \"gradient_accumulation_steps\": training_args.gradient_accumulation_steps,\n    \"warmup_steps\": training_args.warmup_steps,\n    \"max_steps\": training_args.max_steps,\n    \"learning_rate\": training_args.learning_rate,\n    \"fp16\": training_args.fp16,\n    \"bf16\": training_args.bf16,\n    \"logging_steps\": training_args.logging_steps,\n    \"weight_decay\": training_args.weight_decay,\n    \"lr_scheduler_type\": training_args.lr_scheduler_type,\n    \"seed\": training_args.seed,\n    \"output_dir\": training_args.output_dir,\n})\n\nmlflow.log_metric(\"final_loss\", trainer.state.log_history[-1]['train_loss'])\n","metadata":{"execution":{"iopub.status.busy":"2024-06-14T03:27:20.244151Z","iopub.execute_input":"2024-06-14T03:27:20.244754Z","iopub.status.idle":"2024-06-14T03:27:23.343239Z","shell.execute_reply.started":"2024-06-14T03:27:20.244719Z","shell.execute_reply":"2024-06-14T03:27:23.341553Z"},"trusted":true},"execution_count":4,"outputs":[{"name":"stdout","text":"Collecting mlflow\n  Downloading mlflow-2.13.2-py3-none-any.whl.metadata (29 kB)\nRequirement already satisfied: datasets in /opt/conda/lib/python3.10/site-packages (2.19.2)\nCollecting datasets\n  Downloading datasets-2.20.0-py3-none-any.whl.metadata (19 kB)\nRequirement already satisfied: transformers in /opt/conda/lib/python3.10/site-packages (4.41.2)\nCollecting peft\n  Downloading peft-0.11.1-py3-none-any.whl.metadata (13 kB)\n\u001b[31mERROR: Could not find a version that satisfies the requirement unsloth (from versions: none)\u001b[0m\u001b[31m\n\u001b[0m\u001b[31mERROR: No matching distribution found for unsloth\u001b[0m\u001b[31m\n\u001b[0m","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)","Cell \u001b[0;32mIn[4], line 5\u001b[0m\n\u001b[1;32m      2\u001b[0m get_ipython()\u001b[38;5;241m.\u001b[39msystem(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpip install mlflow datasets transformers peft unsloth trl evaluate torch --upgrade --no-cache-dir\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# Import required libraries\u001b[39;00m\n\u001b[0;32m----> 5\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmlflow\u001b[39;00m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01munsloth\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m FastLanguageModel\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\n","\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'mlflow'"],"ename":"ModuleNotFoundError","evalue":"No module named 'mlflow'","output_type":"error"}]},{"cell_type":"code","source":"!nvidia-smi","metadata":{"execution":{"iopub.status.busy":"2024-06-14T03:27:23.344165Z","iopub.status.idle":"2024-06-14T03:27:23.344519Z","shell.execute_reply.started":"2024-06-14T03:27:23.344345Z","shell.execute_reply":"2024-06-14T03:27:23.344362Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"del trainer\ndel sst5\ntorch.cuda.empty_cache()\n\n!nvidia-smi","metadata":{"execution":{"iopub.status.busy":"2024-06-14T03:27:23.345689Z","iopub.status.idle":"2024-06-14T03:27:23.346034Z","shell.execute_reply.started":"2024-06-14T03:27:23.345860Z","shell.execute_reply":"2024-06-14T03:27:23.345874Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import accelerate\nfrom datasets import load_dataset\nfrom transformers import AutoTokenizer\nimport torch\nimport numpy as np\nfrom sklearn.metrics import confusion_matrix, classification_report\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Initialize the accelerator\naccelerator = accelerate.Accelerator()\n\n# Function to get predictions from vllm using batch processing\ndef get_vllm_predictions(model, tokenizer, dataset, batch_size=32):\n    model.eval()\n    predictions, labels = [], []\n    for i in range(0, len(dataset), batch_size):\n        batch = dataset[i:i+batch_size]\n        inputs = tokenizer(batch[\"text\"], return_tensors=\"pt\", max_length=max_seq_length, truncation=True, padding=True).to(device)\n        with torch.no_grad():\n            outputs = model(**inputs)\n        logits = outputs.logits\n        preds = torch.argmax(logits, dim=-1).cpu().numpy()\n        predictions.extend(preds)\n        labels.extend(batch[\"label\"])\n    return predictions, labels\n\n# Load tokenizer and model (assuming they are defined earlier in the script)\n# tokenizer = AutoTokenizer.from_pretrained(\"model_name\")\n# model = SomeModelClass.from_pretrained(\"model_name\")\n\n# Load validation and test sets and group into 3 labels\nval_set = load_dataset(\"SetFit/sst5\", split=\"validation\").map(lambda example: {\"label\": 2 if example[\"label\"] > 3 else (0 if example[\"label\"] < 2 else 1)})\ntest_set = load_dataset(\"SetFit/sst5\", split=\"test\").map(lambda example: {\"label\": 2 if example[\"label\"] > 3 else (0 if example[\"label\"] < 2 else 1)})\n\n# Ensure labels are integers\nval_set = val_set.map(lambda example: {\"label\": int(example[\"label\"])})\ntest_set = test_set.map(lambda example: {\"label\": int(example[\"label\"])})\n\n# Move model to the correct device\nmodel = accelerator.prepare(model)\n\n# Get predictions\nval_predictions, val_labels = get_vllm_predictions(model, tokenizer, val_set)\ntest_predictions, test_labels = get_vllm_predictions(model, tokenizer, test_set)\n\n# Ensure predictions and labels are numpy arrays of integer type\nval_predictions = np.array(val_predictions, dtype=int)\nval_labels = np.array(val_labels, dtype=int)\ntest_predictions = np.array(test_predictions, dtype=int)\ntest_labels = np.array(test_labels, dtype=int)\n\n# Check for consistency in the data types and lengths\nprint(\"Validation Labels Type:\", val_labels.dtype, \"Length:\", len(val_labels))\nprint(\"Validation Predictions Type:\", val_predictions.dtype, \"Length:\", len(val_predictions))\nprint(\"Test Labels Type:\", test_labels.dtype, \"Length:\", len(test_labels))\nprint(\"Test Predictions Type:\", test_predictions.dtype, \"Length:\", len(test_predictions))\n\n# Calculate confusion matrix and classification report\nval_conf_matrix = confusion_matrix(val_labels, val_predictions)\ntest_conf_matrix = confusion_matrix(test_labels, test_predictions)\n\n# Plot confusion matrix for validation set\nplt.figure(figsize=(10, 7))\nsns.heatmap(val_conf_matrix, annot=True, fmt='d', cmap='Blues', xticklabels=[\"Negative\", \"Neutral\", \"Positive\"], yticklabels=[\"Negative\", \"Neutral\", \"Positive\"])\nplt.xlabel('Predicted')\nplt.ylabel('Actual')\nplt.title('Validation Set Confusion Matrix')\nplt.show()\n\n# Plot confusion matrix for test set\nplt.figure(figsize=(10, 7))\nsns.heatmap(test_conf_matrix, annot=True, fmt='d', cmap='Blues', xticklabels=[\"Negative\", \"Neutral\", \"Positive\"], yticklabels=[\"Negative\", \"Neutral\", \"Positive\"])\nplt.xlabel('Predicted')\nplt.ylabel('Actual')\nplt.title('Test Set Confusion Matrix')\nplt.show()\n\n# Print classification reports\nprint(\"Validation Set Classification Report:\")\nprint(classification_report(val_labels, val_predictions, target_names=[\"Negative\", \"Neutral\", \"Positive\"]))\n\nprint(\"Test Set Classification Report:\")\nprint(classification_report(test_labels, test_predictions, target_names=[\"Negative\", \"Neutral\", \"Positive\"]))\n","metadata":{"execution":{"iopub.status.busy":"2024-06-14T03:27:23.347808Z","iopub.status.idle":"2024-06-14T03:27:23.348142Z","shell.execute_reply.started":"2024-06-14T03:27:23.347980Z","shell.execute_reply":"2024-06-14T03:27:23.347993Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"val_predictions = np.array(val_predictions, dtype=int)","metadata":{"execution":{"iopub.status.busy":"2024-06-14T03:27:23.349158Z","iopub.status.idle":"2024-06-14T03:27:23.349487Z","shell.execute_reply.started":"2024-06-14T03:27:23.349326Z","shell.execute_reply":"2024-06-14T03:27:23.349339Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"val_predictions","metadata":{"execution":{"iopub.status.busy":"2024-06-14T03:27:23.351172Z","iopub.status.idle":"2024-06-14T03:27:23.351478Z","shell.execute_reply.started":"2024-06-14T03:27:23.351324Z","shell.execute_reply":"2024-06-14T03:27:23.351336Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}