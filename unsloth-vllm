{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[],"dockerImageVersionId":30732,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install -U nbformat --no-cache-dir","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!pip install -U triton --no-cache-dir","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!pip install -U accelerate --no-cache-dir","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%capture\n# Installs Unsloth, Xformers (Flash Attention) and all other packages!\n!pip install \"unsloth[colab-new] @ git+https://github.com/unslothai/unsloth.git\"\n!pip install --no-deps \"trl<0.9.0\" peft accelerate bitsandbytes --no-cache-dir","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!pip install -U xformers --no-cache-dir","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!pip install -U torch torchvision torchaudio --no-cache-dir","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import torch\nprint(torch.cuda.is_available())","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!nvcc --version","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!export OMP_NUM_THREADS=1\n!export MKL_NUM_THREADS=1","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from unsloth import FastLanguageModel\nimport torch\nmax_seq_length = 2048 # Choose any! We auto support RoPE Scaling internally!\ndtype = None # None for auto detection. Float16 for Tesla T4, V100, Bfloat16 for Ampere+\nload_in_4bit = True # Use 4bit quantization to reduce memory usage. Can be False.\n\n# 4bit pre quantized models we support for 4x faster downloading + no OOMs.\nfourbit_models = [\n    \"unsloth/mistral-7b-v0.3-bnb-4bit\",      # New Mistral v3 2x faster!\n    \"unsloth/mistral-7b-instruct-v0.3-bnb-4bit\",\n    \"unsloth/llama-3-8b-bnb-4bit\",           # Llama-3 15 trillion tokens model 2x faster!\n    \"unsloth/llama-3-8b-Instruct-bnb-4bit\",\n    \"unsloth/llama-3-70b-bnb-4bit\",\n    \"unsloth/Phi-3-mini-4k-instruct\",        # Phi-3 2x faster!\n    \"unsloth/Phi-3-medium-4k-instruct\",\n    \"unsloth/mistral-7b-bnb-4bit\",\n    \"unsloth/gemma-7b-bnb-4bit\",             # Gemma 2.2x faster!\n] # More models at https://huggingface.co/unsloth\n\nmodel, tokenizer = FastLanguageModel.from_pretrained(\n    model_name = \"meta-llama/Meta-Llama-3-8B\",\n    max_seq_length = max_seq_length,\n    dtype = dtype,\n    load_in_4bit = load_in_4bit,\n    token = \"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\", # use one if using gated models like meta-llama/Llama-2-7b-hf\n)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model = FastLanguageModel.get_peft_model(\n    model,\n    r = 16, # Choose any number > 0 ! Suggested 8, 16, 32, 64, 128\n    target_modules = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n                      \"gate_proj\", \"up_proj\", \"down_proj\",],\n    lora_alpha = 16,\n    lora_dropout = 0, # Supports any, but = 0 is optimized\n    bias = \"none\",    # Supports any, but = \"none\" is optimized\n    # [NEW] \"unsloth\" uses 30% less VRAM, fits 2x larger batch sizes!\n    use_gradient_checkpointing = \"unsloth\", # True or \"unsloth\" for very long context\n    random_state = 3407,\n    use_rslora = False,  # We support rank stabilized LoRA\n    loftq_config = None, # And LoftQ\n)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"alpaca_prompt = \"\"\"Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n\n### Instruction:\n{}\n\n### Input:\n{}\n\n### Response:\n{}\"\"\"\n\nEOS_TOKEN = tokenizer.eos_token # Must add EOS_TOKEN\ndef formatting_prompts_func(examples):\n    instructions = examples[\"instruction\"]\n    inputs       = examples[\"input\"]\n    outputs      = examples[\"output\"]\n    texts = []\n    for instruction, input, output in zip(instructions, inputs, outputs):\n        # Must add EOS_TOKEN, otherwise your generation will go on forever!\n        text = alpaca_prompt.format(instruction, input, output) + EOS_TOKEN\n        texts.append(text)\n    return { \"text\" : texts, }\npass\n\nfrom datasets import load_dataset\ndataset = load_dataset(\"yahma/alpaca-cleaned\", split = \"train\")\ndataset = dataset.map(formatting_prompts_func, batched = True,)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!pip install mlflow -U --no-cache-dir","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import os\nos.getcwd()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!ls -a","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!mkdir mlruns","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import mlflow\nmlflow.set_tracking_uri(\"mlruns\")\nmlflow.set_experiment(\"unsloth\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from trl import SFTTrainer\nfrom transformers import TrainingArguments\nfrom unsloth import is_bfloat16_supported\n\ntrainer = SFTTrainer(\n    model = model,\n    tokenizer = tokenizer,\n    train_dataset = dataset,\n    dataset_text_field = \"text\",\n    max_seq_length = max_seq_length,\n    dataset_num_proc = 2,\n    packing = False, # Can make training 5x faster for short sequences.\n    args = TrainingArguments(\n        per_device_train_batch_size = 2,\n        gradient_accumulation_steps = 4,\n        warmup_steps = 5,\n        max_steps = 2,\n        learning_rate = 2e-4,\n        fp16 = not is_bfloat16_supported(),\n        bf16 = is_bfloat16_supported(),\n        logging_steps = 1,\n        optim = \"adamw_8bit\",\n        weight_decay = 0.01,\n        lr_scheduler_type = \"linear\",\n        seed = 3407,\n        output_dir = \"outputs\",\n    ),\n)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# # Install MLflow\n# !pip install mlflow --upgrade --no-cache-dir\n\n# Import required libraries\nimport mlflow\nfrom unsloth import FastLanguageModel\nimport torch\nfrom datasets import load_dataset\nfrom transformers import TrainingArguments\nfrom unsloth import is_bfloat16_supported\nfrom trl import SFTTrainer\n\n# # Initialize MLflow\n# mlflow.set_tracking_uri(\"file:///mnt/data/mlruns\")\n# mlflow.set_experiment(\"your_experiment_name\")\n\n# # Set up model configuration\n# max_seq_length = 2048 # Choose any! We auto support RoPE Scaling internally!\n# dtype = None # None for auto detection. Float16 for Tesla T4, V100, Bfloat16 for Ampere+\n# load_in_4bit = True # Use 4bit quantization to reduce memory usage. Can be False.\n\n# # 4bit pre-quantized models\n# fourbit_models = [\n#     \"unsloth/mistral-7b-v0.3-bnb-4bit\",      # New Mistral v3 2x faster!\n#     \"unsloth/mistral-7b-instruct-v0.3-bnb-4bit\",\n#     \"unsloth/llama-3-8b-bnb-4bit\",           # Llama-3 15 trillion tokens model 2x faster!\n#     \"unsloth/llama-3-8b-Instruct-bnb-4bit\",\n#     \"unsloth/llama-3-70b-bnb-4bit\",\n#     \"unsloth/Phi-3-mini-4k-instruct\",        # Phi-3 2x faster!\n#     \"unsloth/Phi-3-medium-4k-instruct\",\n#     \"unsloth/mistral-7b-bnb-4bit\",\n#     \"unsloth/gemma-7b-bnb-4bit\",             # Gemma 2.2x faster!\n# ] # More models at https://huggingface.co/unsloth\n\n# # Load model and tokenizer\n# model, tokenizer = FastLanguageModel.from_pretrained(\n#     model_name=\"meta-llama/Meta-Llama-3-8B\",\n#     max_seq_length=max_seq_length,\n#     dtype=dtype,\n#     load_in_4bit=load_in_4bit,\n#     token=\"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\", # use one if using gated models like meta-llama/Llama-2-7b-hf\n# )\n# model = FastLanguageModel.get_peft_model(\n#     model,\n#     r=16, # Choose any number > 0 ! Suggested 8, 16, 32, 64, 128\n#     target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n#                     \"gate_proj\", \"up_proj\", \"down_proj\"],\n#     lora_alpha=16,\n#     lora_dropout=0, # Supports any, but = 0 is optimized\n#     bias=\"none\",    # Supports any, but = \"none\" is optimized\n#     use_gradient_checkpointing=\"unsloth\", # True or \"unsloth\" for very long context\n#     random_state=3407,\n#     use_rslora=False,  # We support rank stabilized LoRA\n#     loftq_config=None, # And LoftQ\n# )\n\n# Define prompt and EOS token\nalpaca_prompt = \"\"\"Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n\n### Instruction:\n{}\n\n### Input:\n{}\n\n### Response:\n{}\"\"\"\nEOS_TOKEN = tokenizer.eos_token\n\n# Formatting prompts function\ndef formatting_prompts_func(examples):\n    instructions = examples[\"instruction\"]\n    inputs = examples[\"input\"]\n    outputs = examples[\"output\"]\n    texts = []\n    for instruction, input, output in zip(instructions, inputs, outputs):\n        text = alpaca_prompt.format(instruction, input, output) + EOS_TOKEN\n        texts.append(text)\n    return {\"text\": texts}\n\n# Load and format dataset\ndataset = load_dataset(\"yahma/alpaca-cleaned\", split=\"train\")\ndataset = dataset.map(formatting_prompts_func, batched=True)\n\n# Define training arguments\ntraining_args = TrainingArguments(\n    per_device_train_batch_size=2,\n    gradient_accumulation_steps=4,\n    warmup_steps=5,\n    max_steps=5,\n    learning_rate=2e-4,\n    fp16=not is_bfloat16_supported(),\n    bf16=is_bfloat16_supported(),\n    logging_steps=1,\n    optim=\"adamw_8bit\",\n    weight_decay=0.01,\n    lr_scheduler_type=\"linear\",\n    seed=3407,\n    output_dir=\"outputs\",\n)\n\n# Initialize trainer\ntrainer = SFTTrainer(\n    model=model,\n    tokenizer=tokenizer,\n    train_dataset=dataset,\n    dataset_text_field=\"text\",\n    max_seq_length=max_seq_length,\n    dataset_num_proc=2,\n    packing=False, # Can make training 5x faster for short sequences.\n    args=training_args,\n)\n\ntrainer.train()\n\n# Log parameters and metrics\nmlflow.log_params({\n    \"per_device_train_batch_size\": training_args.per_device_train_batch_size,\n    \"gradient_accumulation_steps\": training_args.gradient_accumulation_steps,\n    \"warmup_steps\": training_args.warmup_steps,\n    \"max_steps\": training_args.max_steps,\n    \"learning_rate\": training_args.learning_rate,\n    \"fp16\": training_args.fp16,\n    \"bf16\": training_args.bf16,\n    \"logging_steps\": training_args.logging_steps,\n    \"weight_decay\": training_args.weight_decay,\n    \"lr_scheduler_type\": training_args.lr_scheduler_type,\n    \"seed\": training_args.seed,\n    \"output_dir\": training_args.output_dir,\n})\n\nmlflow.log_metric(\"final_loss\", trainer.state.log_history[-1]['train_loss'])\n\nimport plotly.graph_objects as go\n\n# Assuming trainer.state.log_history contains the loss values\n# Extract loss values and steps (or epochs) into lists\nloss_values = [log['train_loss'] for log in trainer.state.log_history if 'train_loss' in log]\nsteps = list(range(1, len(loss_values) + 1))\n\n# Create an interactive plot\nfig = go.Figure()\n\n# Add the loss plot\nfig.add_trace(go.Scatter(x=steps, y=loss_values, mode='lines+markers', name='Train Loss'))\n\n# Customize the layout\nfig.update_layout(title='Training Loss Over Time',\n                  xaxis_title='Step',\n                  yaxis_title='Loss',\n                  template='plotly_dark')  # Using a dark theme for better visibility\n\n# Show the plot\nfig.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# alpaca_prompt = Copied from above\nFastLanguageModel.for_inference(model) # Enable native 2x faster inference\ninputs = tokenizer(\n[\n    alpaca_prompt.format(\n        \"Continue the fibonnaci sequence till the maximum you can.\", # instruction\n        \"1, 1, 2, 3, 5, 8\", # input\n        \"\", # output - leave this blank for generation!\n    )\n], return_tensors = \"pt\").to(\"cuda\")\n\nfrom transformers import TextStreamer\ntext_streamer = TextStreamer(tokenizer)\n_ = model.generate(**inputs, streamer = text_streamer, max_new_tokens = 128)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"if False:\n    from unsloth import FastLanguageModel\n    model, tokenizer = FastLanguageModel.from_pretrained(\n        model_name = \"lora_model\", # YOUR MODEL YOU USED FOR TRAINING\n        max_seq_length = max_seq_length,\n        dtype = dtype,\n        load_in_4bit = load_in_4bit,\n    )\n    FastLanguageModel.for_inference(model) # Enable native 2x faster inference\n\n# alpaca_prompt = You MUST copy from above!\n\ninputs = tokenizer(\n[\n    alpaca_prompt.format(\n        \"What is a famous tall tower in Paris?\", # instruction\n        \"\", # input\n        \"\", # output - leave this blank for generation!\n    )\n], return_tensors = \"pt\").to(\"cuda\")\n\noutputs = model.generate(**inputs, max_new_tokens = 64, use_cache = True)\ntokenizer.batch_decode(outputs)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!nvidia-smi","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import torch\n\nprint(f\"CUDA available: {torch.cuda.is_available()}\")\nprint(f\"Number of GPUs: {torch.cuda.device_count()}\")\nfor i in range(torch.cuda.device_count()):\n    print(f\"GPU {i}: {torch.cuda.get_device_name(i)}\")\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!echo $CUDA_VISIBLE_DEVICES","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#@title Show current memory stats\ngpu_stats = torch.cuda.get_device_properties(0)\nstart_gpu_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)\nmax_memory = round(gpu_stats.total_memory / 1024 / 1024 / 1024, 3)\nprint(f\"GPU = {gpu_stats.name}. Max memory = {max_memory} GB.\")\nprint(f\"{start_gpu_memory} GB of memory reserved.\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"break","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Unsloth vLLM on SST5","metadata":{}},{"cell_type":"markdown","source":"## Install Libraries from scratch","metadata":{}},{"cell_type":"code","source":"# # Install Unsloth, Xformers, and other dependencies\n# !pip install \"unsloth[colab-new] @ git+https://github.com/unslothai/unsloth.git\" --no-cache-dir\n\n# # Install necessary libraries without dependencies\n# !pip install --no-deps -U mlflow datasets transformers peft accelerate trl evaluate autoawq seaborn --no-cache-dir\n\n# # Check CUDA version and NVIDIA driver status\n# !nvcc --version\n# !nvidia-smi\n\n# # Install PyTorch and related packages\n# !pip install -U torch torchvision torchaudio --no-cache-dir\n\n# # Set environment variables for optimized performance\n# !export OMP_NUM_THREADS=1\n# !export MKL_NUM_THREADS=1\n\n# # Verify CUDA availability in PyTorch\n# import torch\n\n# device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n# print(device)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Install libraries from cache","metadata":{}},{"cell_type":"code","source":"# Install Unsloth, Xformers, and other dependencies\n!pip install \"unsloth[colab-new] @ git+https://github.com/unslothai/unsloth.git\"\n\n# Install necessary libraries without dependencies\n!pip install --no-deps mlflow datasets transformers peft accelerate trl evaluate autoawq seaborn\n\n# Check CUDA version and NVIDIA driver status\n!nvcc --version\n!nvidia-smi\n\n# Install PyTorch and related packages\n!pip install torch torchvision torchaudio\n\n# Set environment variables for optimized performance\n!export OMP_NUM_THREADS=1\n!export MKL_NUM_THREADS=1\n\n# Verify CUDA availability in PyTorch\nimport torch\n\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nprint(device)\n","metadata":{"execution":{"iopub.status.busy":"2024-06-14T07:59:45.709888Z","iopub.execute_input":"2024-06-14T07:59:45.710562Z","iopub.status.idle":"2024-06-14T08:00:53.159952Z","shell.execute_reply.started":"2024-06-14T07:59:45.710533Z","shell.execute_reply":"2024-06-14T08:00:53.159027Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"Collecting unsloth@ git+https://github.com/unslothai/unsloth.git (from unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git)\n  Cloning https://github.com/unslothai/unsloth.git to /tmp/pip-install-151h1c1_/unsloth_a59dfcbcd5ed4faabaefa07f26816b34\n  Running command git clone --filter=blob:none --quiet https://github.com/unslothai/unsloth.git /tmp/pip-install-151h1c1_/unsloth_a59dfcbcd5ed4faabaefa07f26816b34\n  Resolved https://github.com/unslothai/unsloth.git to commit 7ac1a78904802a66e6e799f03b6f0c910cd0735a\n  Installing build dependencies ... \u001b[?25ldone\n\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n\u001b[?25h  Installing backend dependencies ... \u001b[?25ldone\n\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n\u001b[?25hCollecting tyro (from unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git)\n  Downloading tyro-0.8.4-py3-none-any.whl.metadata (7.9 kB)\nRequirement already satisfied: transformers>=4.38.2 in /opt/conda/lib/python3.10/site-packages (from unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (4.41.2)\nRequirement already satisfied: datasets>=2.16.0 in /opt/conda/lib/python3.10/site-packages (from unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (2.19.2)\nRequirement already satisfied: sentencepiece in /opt/conda/lib/python3.10/site-packages (from unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (0.2.0)\nRequirement already satisfied: tqdm in /opt/conda/lib/python3.10/site-packages (from unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (4.66.4)\nRequirement already satisfied: psutil in /opt/conda/lib/python3.10/site-packages (from unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (5.9.3)\nRequirement already satisfied: wheel>=0.42.0 in /opt/conda/lib/python3.10/site-packages (from unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (0.42.0)\nRequirement already satisfied: numpy in /opt/conda/lib/python3.10/site-packages (from unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (1.26.4)\nRequirement already satisfied: protobuf<4.0.0 in /opt/conda/lib/python3.10/site-packages (from unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (3.20.3)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from datasets>=2.16.0->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (3.13.1)\nRequirement already satisfied: pyarrow>=12.0.0 in /opt/conda/lib/python3.10/site-packages (from datasets>=2.16.0->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (14.0.2)\nRequirement already satisfied: pyarrow-hotfix in /opt/conda/lib/python3.10/site-packages (from datasets>=2.16.0->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (0.6)\nRequirement already satisfied: dill<0.3.9,>=0.3.0 in /opt/conda/lib/python3.10/site-packages (from datasets>=2.16.0->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (0.3.8)\nRequirement already satisfied: pandas in /opt/conda/lib/python3.10/site-packages (from datasets>=2.16.0->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (2.2.1)\nRequirement already satisfied: requests>=2.32.1 in /opt/conda/lib/python3.10/site-packages (from datasets>=2.16.0->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (2.32.3)\nRequirement already satisfied: xxhash in /opt/conda/lib/python3.10/site-packages (from datasets>=2.16.0->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (3.4.1)\nRequirement already satisfied: multiprocess in /opt/conda/lib/python3.10/site-packages (from datasets>=2.16.0->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (0.70.16)\nRequirement already satisfied: fsspec<=2024.3.1,>=2023.1.0 in /opt/conda/lib/python3.10/site-packages (from fsspec[http]<=2024.3.1,>=2023.1.0->datasets>=2.16.0->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (2024.3.1)\nRequirement already satisfied: aiohttp in /opt/conda/lib/python3.10/site-packages (from datasets>=2.16.0->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (3.9.1)\nRequirement already satisfied: huggingface-hub>=0.21.2 in /opt/conda/lib/python3.10/site-packages (from datasets>=2.16.0->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (0.23.2)\nRequirement already satisfied: packaging in /opt/conda/lib/python3.10/site-packages (from datasets>=2.16.0->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (21.3)\nRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from datasets>=2.16.0->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (6.0.1)\nRequirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.10/site-packages (from transformers>=4.38.2->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (2023.12.25)\nRequirement already satisfied: tokenizers<0.20,>=0.19 in /opt/conda/lib/python3.10/site-packages (from transformers>=4.38.2->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (0.19.1)\nRequirement already satisfied: safetensors>=0.4.1 in /opt/conda/lib/python3.10/site-packages (from transformers>=4.38.2->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (0.4.3)\nRequirement already satisfied: docstring-parser>=0.14.1 in /opt/conda/lib/python3.10/site-packages (from tyro->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (0.15)\nRequirement already satisfied: typing-extensions>=4.7.0 in /opt/conda/lib/python3.10/site-packages (from tyro->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (4.9.0)\nRequirement already satisfied: rich>=11.1.0 in /opt/conda/lib/python3.10/site-packages (from tyro->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (13.7.0)\nCollecting shtab>=1.5.6 (from tyro->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git)\n  Downloading shtab-1.7.1-py3-none-any.whl.metadata (7.3 kB)\nRequirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets>=2.16.0->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (23.2.0)\nRequirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets>=2.16.0->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (6.0.4)\nRequirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets>=2.16.0->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (1.9.3)\nRequirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets>=2.16.0->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (1.4.1)\nRequirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets>=2.16.0->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (1.3.1)\nRequirement already satisfied: async-timeout<5.0,>=4.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets>=2.16.0->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (4.0.3)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging->datasets>=2.16.0->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (3.1.1)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests>=2.32.1->datasets>=2.16.0->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests>=2.32.1->datasets>=2.16.0->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (3.6)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests>=2.32.1->datasets>=2.16.0->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests>=2.32.1->datasets>=2.16.0->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (2024.2.2)\nRequirement already satisfied: markdown-it-py>=2.2.0 in /opt/conda/lib/python3.10/site-packages (from rich>=11.1.0->tyro->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (3.0.0)\nRequirement already satisfied: pygments<3.0.0,>=2.13.0 in /opt/conda/lib/python3.10/site-packages (from rich>=11.1.0->tyro->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (2.17.2)\nRequirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets>=2.16.0->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (2.9.0.post0)\nRequirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets>=2.16.0->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (2023.3.post1)\nRequirement already satisfied: tzdata>=2022.7 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets>=2.16.0->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (2023.4)\nRequirement already satisfied: mdurl~=0.1 in /opt/conda/lib/python3.10/site-packages (from markdown-it-py>=2.2.0->rich>=11.1.0->tyro->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (0.1.2)\nRequirement already satisfied: six>=1.5 in /opt/conda/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas->datasets>=2.16.0->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (1.16.0)\nDownloading tyro-0.8.4-py3-none-any.whl (102 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m102.4/102.4 kB\u001b[0m \u001b[31m1.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m \u001b[36m0:00:01\u001b[0m\n\u001b[?25hDownloading shtab-1.7.1-py3-none-any.whl (14 kB)\nBuilding wheels for collected packages: unsloth\n  Building wheel for unsloth (pyproject.toml) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for unsloth: filename=unsloth-2024.6-py3-none-any.whl size=116247 sha256=4328c9a038a511b18d166820c0ff851e6e952b18742574b798df46fa812b0ef6\n  Stored in directory: /tmp/pip-ephem-wheel-cache-2f6gpdj9/wheels/ed/d4/e9/76fb290ee3df0a5fc21ce5c2c788e29e9607a2353d8342fd0d\nSuccessfully built unsloth\nInstalling collected packages: unsloth, shtab, tyro\nSuccessfully installed shtab-1.7.1 tyro-0.8.4 unsloth-2024.6\nCollecting mlflow\n  Downloading mlflow-2.13.2-py3-none-any.whl.metadata (29 kB)\nRequirement already satisfied: datasets in /opt/conda/lib/python3.10/site-packages (2.19.2)\nRequirement already satisfied: transformers in /opt/conda/lib/python3.10/site-packages (4.41.2)\nCollecting peft\n  Downloading peft-0.11.1-py3-none-any.whl.metadata (13 kB)\nRequirement already satisfied: accelerate in /opt/conda/lib/python3.10/site-packages (0.30.1)\nCollecting trl\n  Downloading trl-0.9.4-py3-none-any.whl.metadata (11 kB)\nCollecting evaluate\n  Downloading evaluate-0.4.2-py3-none-any.whl.metadata (9.3 kB)\nCollecting autoawq\n  Downloading autoawq-0.2.5-cp310-cp310-manylinux2014_x86_64.whl.metadata (16 kB)\nRequirement already satisfied: seaborn in /opt/conda/lib/python3.10/site-packages (0.12.2)\nDownloading mlflow-2.13.2-py3-none-any.whl (25.0 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m25.0/25.0 MB\u001b[0m \u001b[31m51.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading peft-0.11.1-py3-none-any.whl (251 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m251.6/251.6 kB\u001b[0m \u001b[31m18.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading trl-0.9.4-py3-none-any.whl (226 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m226.7/226.7 kB\u001b[0m \u001b[31m17.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading evaluate-0.4.2-py3-none-any.whl (84 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m84.1/84.1 kB\u001b[0m \u001b[31m6.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading autoawq-0.2.5-cp310-cp310-manylinux2014_x86_64.whl (84 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m84.3/84.3 kB\u001b[0m \u001b[31m6.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hInstalling collected packages: trl, peft, mlflow, evaluate, autoawq\nSuccessfully installed autoawq-0.2.5 evaluate-0.4.2 mlflow-2.13.2 peft-0.11.1 trl-0.9.4\nnvcc: NVIDIA (R) Cuda compiler driver\nCopyright (c) 2005-2023 NVIDIA Corporation\nBuilt on Mon_Apr__3_17:16:06_PDT_2023\nCuda compilation tools, release 12.1, V12.1.105\nBuild cuda_12.1.r12.1/compiler.32688072_0\nFri Jun 14 08:00:34 2024       \n+---------------------------------------------------------------------------------------+\n| NVIDIA-SMI 535.129.03             Driver Version: 535.129.03   CUDA Version: 12.2     |\n|-----------------------------------------+----------------------+----------------------+\n| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n|                                         |                      |               MIG M. |\n|=========================================+======================+======================|\n|   0  Tesla T4                       Off | 00000000:00:04.0 Off |                    0 |\n| N/A   43C    P8              11W /  70W |      0MiB / 15360MiB |      0%      Default |\n|                                         |                      |                  N/A |\n+-----------------------------------------+----------------------+----------------------+\n|   1  Tesla T4                       Off | 00000000:00:05.0 Off |                    0 |\n| N/A   44C    P8              12W /  70W |      0MiB / 15360MiB |      0%      Default |\n|                                         |                      |                  N/A |\n+-----------------------------------------+----------------------+----------------------+\n                                                                                         \n+---------------------------------------------------------------------------------------+\n| Processes:                                                                            |\n|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n|        ID   ID                                                             Usage      |\n|=======================================================================================|\n|  No running processes found                                                           |\n+---------------------------------------------------------------------------------------+\nRequirement already satisfied: torch in /opt/conda/lib/python3.10/site-packages (2.1.2)\nRequirement already satisfied: torchvision in /opt/conda/lib/python3.10/site-packages (0.16.2)\nRequirement already satisfied: torchaudio in /opt/conda/lib/python3.10/site-packages (2.1.2)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from torch) (3.13.1)\nRequirement already satisfied: typing-extensions in /opt/conda/lib/python3.10/site-packages (from torch) (4.9.0)\nRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch) (1.12.1)\nRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch) (3.2.1)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch) (3.1.2)\nRequirement already satisfied: fsspec in /opt/conda/lib/python3.10/site-packages (from torch) (2024.3.1)\nRequirement already satisfied: numpy in /opt/conda/lib/python3.10/site-packages (from torchvision) (1.26.4)\nRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from torchvision) (2.32.3)\nRequirement already satisfied: pillow!=8.3.*,>=5.3.0 in /opt/conda/lib/python3.10/site-packages (from torchvision) (9.5.0)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch) (2.1.3)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->torchvision) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->torchvision) (3.6)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->torchvision) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->torchvision) (2024.2.2)\nRequirement already satisfied: mpmath<1.4.0,>=1.1.0 in /opt/conda/lib/python3.10/site-packages (from sympy->torch) (1.3.0)\ncuda\n","output_type":"stream"}]},{"cell_type":"markdown","source":"## Set Huggingface token","metadata":{}},{"cell_type":"code","source":"import os\n\nos.environ['hf_token'] = \"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\"\n\nfrom dotenv import load_dotenv\n\nload_dotenv()  # take environment variables from .env.\n\nhf_token = os.getenv('hf_token')\n\nhf_token","metadata":{"execution":{"iopub.status.busy":"2024-06-14T09:22:12.281114Z","iopub.execute_input":"2024-06-14T09:22:12.281782Z","iopub.status.idle":"2024-06-14T09:22:12.288946Z","shell.execute_reply.started":"2024-06-14T09:22:12.281749Z","shell.execute_reply":"2024-06-14T09:22:12.288040Z"},"trusted":true},"execution_count":28,"outputs":[{"execution_count":28,"output_type":"execute_result","data":{"text/plain":"'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'"},"metadata":{}}]},{"cell_type":"markdown","source":"## Quantize own model if required","metadata":{}},{"cell_type":"code","source":"import os\nos.getcwd()","metadata":{"execution":{"iopub.status.busy":"2024-06-14T08:15:08.554264Z","iopub.execute_input":"2024-06-14T08:15:08.554644Z","iopub.status.idle":"2024-06-14T08:15:08.560569Z","shell.execute_reply.started":"2024-06-14T08:15:08.554616Z","shell.execute_reply":"2024-06-14T08:15:08.559645Z"},"trusted":true},"execution_count":15,"outputs":[{"execution_count":15,"output_type":"execute_result","data":{"text/plain":"'/kaggle/working'"},"metadata":{}}]},{"cell_type":"code","source":"import os\nfrom awq import AutoAWQForCausalLM\nfrom transformers import AutoTokenizer\n\n# Load the token from the environment variable\nhf_token = os.getenv(hf_token)\n\n# Define the model name or path\nmodel_path = \"meta-llama/Meta-Llama-3-8B\"  # Replace with your model name\n\nquant_name = 'quantized_llama3-8b-awq'\n\nquant_path = \"/kaggle/working/\" + quant_name\n\nquant_config = {\"zero_point\": True, \"q_group_size\": 128, \"w_bit\": 4}\n                \n\n# Load model\nmodel = AutoAWQForCausalLM.from_pretrained(model_path, device_map = 'auto')\ntokenizer = AutoTokenizer.from_pretrained(model_path, trust_remote_code = True)\n\n# Quantize\nmodel.quantize(tokenizer, quant_config = quant_config)\n\n# Save quantized model\nmodel.save_quantized(quant_name, safetensors = True)\ntokenizer.save_pretrained(quant_name)","metadata":{"execution":{"iopub.status.busy":"2024-06-14T08:19:47.188005Z","iopub.execute_input":"2024-06-14T08:19:47.188462Z","iopub.status.idle":"2024-06-14T09:09:57.721392Z","shell.execute_reply.started":"2024-06-14T08:19:47.188418Z","shell.execute_reply":"2024-06-14T09:09:57.720396Z"},"trusted":true},"execution_count":21,"outputs":[{"output_type":"display_data","data":{"text/plain":"Fetching 16 files:   0%|          | 0/16 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c33c10fb37fc4ff8825ca962437dc9a5"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e595042ba5ae4c49b9612c1b2e085f7c"}},"metadata":{}},{"name":"stderr","text":"Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Downloading readme:   0%|          | 0.00/167 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0588fe27ac7c47b690b63c73d960eaed"}},"metadata":{}},{"name":"stderr","text":"Repo card metadata block was not found. Setting CardData to empty.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Downloading data:   0%|          | 0.00/471M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f907dbe9e5794a94873e60f658abe3a3"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating validation split:   0%|          | 0/214670 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"230de0d7331545ba91cf4de7da111cfd"}},"metadata":{}},{"name":"stderr","text":"AWQ: 100%|██████████| 32/32 [48:54<00:00, 91.69s/it]\n","output_type":"stream"},{"execution_count":21,"output_type":"execute_result","data":{"text/plain":"('quantized_llama3-8b-awq/tokenizer_config.json',\n 'quantized_llama3-8b-awq/special_tokens_map.json',\n 'quantized_llama3-8b-awq/tokenizer.json')"},"metadata":{}}]},{"cell_type":"code","source":"# Save quantized model\nmodel.save_quantized(quant_path)\ntokenizer.save_pretrained(quant_path)","metadata":{"execution":{"iopub.status.busy":"2024-06-14T09:24:45.732334Z","iopub.execute_input":"2024-06-14T09:24:45.733070Z","iopub.status.idle":"2024-06-14T09:25:11.082315Z","shell.execute_reply.started":"2024-06-14T09:24:45.733039Z","shell.execute_reply":"2024-06-14T09:25:11.081352Z"},"trusted":true},"execution_count":31,"outputs":[{"execution_count":31,"output_type":"execute_result","data":{"text/plain":"('/kaggle/working/quantized_llama3-8b-awq/tokenizer_config.json',\n '/kaggle/working/quantized_llama3-8b-awq/special_tokens_map.json',\n '/kaggle/working/quantized_llama3-8b-awq/tokenizer.json')"},"metadata":{}}]},{"cell_type":"code","source":"import os\nfrom huggingface_hub import HfApi, create_repo, upload_folder\n\n# Set environment variable for Hugging Face token\nhf_token = hf_token\n\n# Define the paths to your model and tokenizer\nmodel_dir = \"/kaggle/working/quantized_llama3-8b-awq\"\nrepo_name = \"rafatsiddiqui/Meta-Llama-3-8B-AWQ\"\n\n## Create a repository on Hugging Face Hub\ncreate_repo(repo_id=repo_name, token=hf_token, exist_ok=True)\n\n# Upload the model directory to the repository\nupload_folder(\n    folder_path=model_dir,\n    path_in_repo=\".\",\n    repo_id=repo_name,\n    token=hf_token\n)\n","metadata":{"execution":{"iopub.status.busy":"2024-06-14T09:29:09.203461Z","iopub.execute_input":"2024-06-14T09:29:09.203824Z","iopub.status.idle":"2024-06-14T09:31:00.242824Z","shell.execute_reply.started":"2024-06-14T09:29:09.203795Z","shell.execute_reply":"2024-06-14T09:31:00.241884Z"},"trusted":true},"execution_count":34,"outputs":[{"output_type":"display_data","data":{"text/plain":"model-00002-of-00002.safetensors:   0%|          | 0.00/1.05G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"af1656e7fe0e4eb59cd808539ea29f5b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00001-of-00002.safetensors:   0%|          | 0.00/4.68G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b0258d3c67394d3099d70b5c6175e39e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Upload 2 LFS files:   0%|          | 0/2 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0b2bccf28e3e4029ba1a978825ae2d96"}},"metadata":{}},{"execution_count":34,"output_type":"execute_result","data":{"text/plain":"CommitInfo(commit_url='https://huggingface.co/rafatsiddiqui/Meta-Llama-3-8B-AWQ/commit/cf75051de8038b89800d78c5037353331ab3d9e3', commit_message='Upload folder using huggingface_hub', commit_description='', oid='cf75051de8038b89800d78c5037353331ab3d9e3', pr_url=None, pr_revision=None, pr_num=None)"},"metadata":{}}]},{"cell_type":"code","source":"import os\nfrom transformers import AutoTokenizer\nfrom awq import AutoAWQForCausalLM\nfrom awq.quantize import AWQConfig\nfrom huggingface_hub import Repository\n\n# Load the Hugging Face token from environment variable\nhf_token = os.getenv(\"hf_token\")\n\n# Save the quantized model and tokenizer\nmodel.save_pretrained(output_dir)\ntokenizer = AutoTokenizer.from_pretrained(model_name_or_path, token=hf_token)\ntokenizer.save_pretrained(output_dir)\n\n# Push the model to Hugging Face Hub\nmodel_name = \"your-username/Meta-Llama-3-8B-AWQ\"\nrepo = Repository(local_dir=output_dir, clone_from=model_name, use_auth_token=hf_token)\nrepo.push_to_hub(commit_message=\"Add quantized model\")\n","metadata":{"execution":{"iopub.status.busy":"2024-06-14T09:22:57.728391Z","iopub.execute_input":"2024-06-14T09:22:57.728769Z","iopub.status.idle":"2024-06-14T09:22:57.775894Z","shell.execute_reply.started":"2024-06-14T09:22:57.728741Z","shell.execute_reply":"2024-06-14T09:22:57.774717Z"},"trusted":true},"execution_count":30,"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)","Cell \u001b[0;32mIn[30], line 4\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtransformers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m AutoTokenizer\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mawq\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m AutoAWQForCausalLM\n\u001b[0;32m----> 4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mawq\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mquantize\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m AWQConfig\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mhuggingface_hub\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Repository\n\u001b[1;32m      7\u001b[0m \u001b[38;5;66;03m# Load the Hugging Face token from environment variable\u001b[39;00m\n","\u001b[0;31mImportError\u001b[0m: cannot import name 'AWQConfig' from 'awq.quantize' (/opt/conda/lib/python3.10/site-packages/awq/quantize/__init__.py)"],"ename":"ImportError","evalue":"cannot import name 'AWQConfig' from 'awq.quantize' (/opt/conda/lib/python3.10/site-packages/awq/quantize/__init__.py)","output_type":"error"}]},{"cell_type":"code","source":"from huggingface_hub import HfApi\n\n# Define model name on the Hub\nmodel_name = \"Meta-Llama-3-8B-AWQ\"\n\n# Login to Hugging Face Hub\n!huggingface-cli login\n\n# Initialize the HfApi\napi = HfApi()\n\n# Create a repository on the Hugging Face Hub\napi.create_repo(model_name)\n\n# Upload the model\napi.upload_folder(\n    folder_path=output_dir,\n    path_in_repo=\".\",\n    repo_id=model_name\n)\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"output_dir = \"./quantized_llama3-8b-awq\"\nmodel.save_pretrained(output_dir)\ntokenizer.save_pretrained(output_dir)\n","metadata":{"execution":{"iopub.status.busy":"2024-06-14T09:17:58.067256Z","iopub.execute_input":"2024-06-14T09:17:58.067890Z","iopub.status.idle":"2024-06-14T09:17:58.116759Z","shell.execute_reply.started":"2024-06-14T09:17:58.067858Z","shell.execute_reply":"2024-06-14T09:17:58.115700Z"},"trusted":true},"execution_count":26,"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","Cell \u001b[0;32mIn[26], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m output_dir \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m./quantized_llama3-8b-awq\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m----> 2\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msave_pretrained\u001b[49m(output_dir)\n\u001b[1;32m      3\u001b[0m tokenizer\u001b[38;5;241m.\u001b[39msave_pretrained(output_dir)\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1695\u001b[0m, in \u001b[0;36mModule.__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   1693\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m modules:\n\u001b[1;32m   1694\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m modules[name]\n\u001b[0;32m-> 1695\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m object has no attribute \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n","\u001b[0;31mAttributeError\u001b[0m: 'LlamaAWQForCausalLM' object has no attribute 'save_pretrained'"],"ename":"AttributeError","evalue":"'LlamaAWQForCausalLM' object has no attribute 'save_pretrained'","output_type":"error"}]},{"cell_type":"code","source":"output_dir = \"./quantized_llama3-8b-awq\"\nmodel.save_pretrained(output_dir)\ntokenizer.save_pretrained(output_dir)\n","metadata":{"execution":{"iopub.status.busy":"2024-06-14T09:17:45.368293Z","iopub.execute_input":"2024-06-14T09:17:45.368649Z","iopub.status.idle":"2024-06-14T09:17:45.833771Z","shell.execute_reply.started":"2024-06-14T09:17:45.368621Z","shell.execute_reply":"2024-06-14T09:17:45.832608Z"},"trusted":true},"execution_count":25,"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","Cell \u001b[0;32mIn[25], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m model\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      2\u001b[0m tokenizer\u001b[38;5;241m.\u001b[39msave_pretrained(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMeta-Llama-3-8B-Instruct-AWQ\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m----> 3\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msave_pretrained\u001b[49m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMeta-Llama-3-8B-Instruct-AWQ\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1695\u001b[0m, in \u001b[0;36mModule.__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   1693\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m modules:\n\u001b[1;32m   1694\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m modules[name]\n\u001b[0;32m-> 1695\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m object has no attribute \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n","\u001b[0;31mAttributeError\u001b[0m: 'LlamaAWQForCausalLM' object has no attribute 'save_pretrained'"],"ename":"AttributeError","evalue":"'LlamaAWQForCausalLM' object has no attribute 'save_pretrained'","output_type":"error"}]},{"cell_type":"code","source":"model.push_to_hub(\"Meta-Llama-3-8B-AWQ\")\ntokenizer.push_to_hub(\"Meta-Llama-3-8B-AWQ\")","metadata":{"execution":{"iopub.status.busy":"2024-06-14T09:17:17.352979Z","iopub.execute_input":"2024-06-14T09:17:17.353548Z","iopub.status.idle":"2024-06-14T09:17:17.406089Z","shell.execute_reply.started":"2024-06-14T09:17:17.353512Z","shell.execute_reply":"2024-06-14T09:17:17.404863Z"},"trusted":true},"execution_count":24,"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","Cell \u001b[0;32mIn[24], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpush_to_hub\u001b[49m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMeta-Llama-3-8B-AWQ\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      2\u001b[0m tokenizer\u001b[38;5;241m.\u001b[39mpush_to_hub(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMeta-Llama-3-8B-AWQ\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1695\u001b[0m, in \u001b[0;36mModule.__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   1693\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m modules:\n\u001b[1;32m   1694\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m modules[name]\n\u001b[0;32m-> 1695\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m object has no attribute \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n","\u001b[0;31mAttributeError\u001b[0m: 'LlamaAWQForCausalLM' object has no attribute 'push_to_hub'"],"ename":"AttributeError","evalue":"'LlamaAWQForCausalLM' object has no attribute 'push_to_hub'","output_type":"error"}]},{"cell_type":"code","source":"model.to(\"cpu\")\ntokenizer.save_pretrained(\"Meta-Llama-3-8B-Instruct-AWQ\")\nmodel.save_pretrained(\"Meta-Llama-3-8B-Instruct-AWQ\")","metadata":{"execution":{"iopub.status.busy":"2024-06-14T09:09:58.721627Z","iopub.status.idle":"2024-06-14T09:09:58.722103Z","shell.execute_reply.started":"2024-06-14T09:09:58.721837Z","shell.execute_reply":"2024-06-14T09:09:58.721858Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"break","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Import libraries","metadata":{}},{"cell_type":"code","source":"print('hello world')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Import required libraries\nimport unsloth\nimport mlflow\nfrom unsloth import FastLanguageModel\nimport torch\nfrom datasets import load_dataset\nfrom transformers import TrainingArguments, AutoTokenizer\nfrom unsloth import is_bfloat16_supported\nfrom trl import SFTTrainer\nimport evaluate\nimport numpy as np\nfrom sklearn.metrics import confusion_matrix, classification_report\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport os\nimport shutil","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# # Install required packages\n# # !pip install -U mlflow datasets transformers peft unsloth trl evaluate torch --no-cache-dir\n\n# # Import required libraries\n# import mlflow\n# from unsloth import FastLanguageModel\n# import torch\n# from datasets import load_dataset\n# from transformers import TrainingArguments, AutoTokenizer\n# from unsloth import is_bfloat16_supported\n# from trl import SFTTrainer\n# import evaluate\n# import numpy as np\n# from sklearn.metrics import confusion_matrix, classification_report\n# import matplotlib.pyplot as plt\n# import seaborn as sns\n\n# # Check if CUDA is available and set device accordingly\n# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n# # Initialize MLflow\n# mlflow.set_tracking_uri(\"file:///mnt/data/mlruns\")\n# mlflow.set_experiment(\"unsloth-train\")\n\n# # Set up model configuration\n# max_seq_length = 2048 # Choose any! We auto support RoPE Scaling internally!\n# dtype = None # None for auto detection. Float16 for Tesla T4, V100, Bfloat16 for Ampere+\n# load_in_4bit = True # Use 4bit quantization to reduce memory usage. Can be False.\n\n# # 4bit pre-quantized models\n# fourbit_models = [\n#     \"unsloth/mistral-7b-v0.3-bnb-4bit\",\n#     \"unsloth/mistral-7b-instruct-v0.3-bnb-4bit\",\n#     \"unsloth/llama-3-8b-bnb-4bit\",\n#     \"unsloth/llama-3-8b-Instruct-bnb-4bit\",\n#     \"unsloth/llama-3-70b-bnb-4bit\",\n#     \"unsloth/Phi-3-mini-4k-instruct\",\n#     \"unsloth/Phi-3-medium-4k-instruct\",\n#     \"unsloth/mistral-7b-bnb-4bit\",\n#     \"unsloth/gemma-7b-bnb-4bit\",\n# ]\n\n# # Load model and tokenizer\n# model, tokenizer = FastLanguageModel.from_pretrained(\n#     model_name=\"meta-llama/Meta-Llama-3-8B\",\n#     max_seq_length=max_seq_length,\n#     dtype=dtype,\n#     load_in_4bit=load_in_4bit,\n#     token=\"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\", # use one if using gated models like meta-llama/Llama-2-7b-hf\n# )\n# model = FastLanguageModel.get_peft_model(\n#     model,\n#     r=16, # Choose any number > 0 ! Suggested 8, 16, 32, 64, 128\n#     target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n#                     \"gate_proj\", \"up_proj\", \"down_proj\"],\n#     lora_alpha=16,\n#     lora_dropout=0, # Supports any, but = 0 is optimized\n#     bias=\"none\",    # Supports any, but = \"none\" is optimized\n#     use_gradient_checkpointing=\"unsloth\", # True or \"unsloth\" for very long context\n#     random_state=3407,\n#     use_rslora=False,  # We support rank stabilized LoRA\n#     loftq_config=None, # And LoftQ\n# ).to(device)\n\n# # Define prompt and EOS token\n# alpaca_prompt = \"\"\"Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n\n# ### Instruction:\n# {}\n\n# ### Input:\n# {}\n\n# ### Response:\n# {}\"\"\"\n# EOS_TOKEN = tokenizer.eos_token\n\n# # Formatting prompts function\n# def formatting_prompts_func(examples):\n#     instructions = examples[\"instruction\"]\n#     inputs = examples[\"input\"]\n#     outputs = examples[\"output\"]\n#     texts = []\n#     for instruction, input, output in zip(instructions, inputs, outputs):\n#         text = alpaca_prompt.format(instruction, input, output) + EOS_TOKEN\n#         texts.append(text)\n#     return {\"text\": texts}\n\n# # Load and format dataset\n# sst5 = load_dataset(\"SetFit/sst5\", split=\"train\")  # Adjusted to SST5 dataset\n# sst5 = sst5.map(lambda example: {\"label\": 2 if example[\"label\"] > 3 else (0 if example[\"label\"] < 2 else 1)})  # Grouping into 3 labels\n\n# # Define training arguments\n# training_args = TrainingArguments(\n#     per_device_train_batch_size=2,\n#     gradient_accumulation_steps=4,\n#     warmup_steps=5,\n#     max_steps=2,\n#     learning_rate=2e-4,\n#     fp16=torch.cuda.is_available() and not is_bfloat16_supported(),\n#     bf16=is_bfloat16_supported(),\n#     logging_steps=1,\n#     optim=\"adamw_8bit\",\n#     weight_decay=0.01,\n#     lr_scheduler_type=\"linear\",\n#     seed=3407,\n#     output_dir=\"outputs\",\n# )\n\n# # Initialize trainer\n# trainer = SFTTrainer(\n#     model=model,\n#     tokenizer=tokenizer,\n#     train_dataset=sst5,\n#     dataset_text_field=\"text\",\n#     max_seq_length=max_seq_length,\n#     dataset_num_proc=2,\n#     packing=False, # Can make training 5x faster for short sequences.\n#     args=training_args,\n# )\n\n# trainer.train()\n\n# # Log parameters and metrics\n# mlflow.log_params({\n#     \"per_device_train_batch_size\": training_args.per_device_train_batch_size,\n#     \"gradient_accumulation_steps\": training_args.gradient_accumulation_steps,\n#     \"warmup_steps\": training_args.warmup_steps,\n#     \"max_steps\": training_args.max_steps,\n#     \"learning_rate\": training_args.learning_rate,\n#     \"fp16\": training_args.fp16,\n#     \"bf16\": training_args.bf16,\n#     \"logging_steps\": training_args.logging_steps,\n#     \"weight_decay\": training_args.weight_decay,\n#     \"lr_scheduler_type\": training_args.lr_scheduler_type,\n#     \"seed\": training_args.seed,\n#     \"output_dir\": training_args.output_dir,\n# })\n\n# mlflow.log_metric(\"final_loss\", trainer.state.log_history[-1]['train_loss'])\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Set up environment with mlflow","metadata":{}},{"cell_type":"code","source":"# Clear cache directory\ncache_dir = os.path.join(os.path.expanduser(\"~\"), \".cache\")\nif os.path.exists(cache_dir):\n    shutil.rmtree(cache_dir)\nos.makedirs(cache_dir)\n\n# Check if CUDA is available and set device accordingly\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n# Initialize MLflow\nmlflow.set_tracking_uri(\"file:///mnt/data/mlruns\")\nmlflow.set_experiment(\"unsloth-train\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Set up model config for import","metadata":{}},{"cell_type":"code","source":"# Set up model configuration\nmax_seq_length = 2048 # Choose any! We auto support RoPE Scaling internally!\ndtype = None # None for auto detection. Float16 for Tesla T4, V100, Bfloat16 for Ampere+\nload_in_4bit = True # Use 4bit quantization to reduce memory usage. Can be False.\n\n# 4bit pre-quantized models\nfourbit_models = [\n    \"unsloth/mistral-7b-v0.3-bnb-4bit\",\n    \"unsloth/mistral-7b-instruct-v0.3-bnb-4bit\",\n    \"unsloth/llama-3-8b-bnb-4bit\",\n    \"unsloth/llama-3-8b-Instruct-bnb-4bit\",\n    \"unsloth/llama-3-70b-bnb-4bit\",\n    \"unsloth/Phi-3-mini-4k-instruct\",\n    \"unsloth/Phi-3-medium-4k-instruct\",\n    \"unsloth/mistral-7b-bnb-4bit\",\n    \"unsloth/gemma-7b-bnb-4bit\",\n]\n\n# Load model and tokenizer\nmodel, tokenizer = FastLanguageModel.from_pretrained(\n    model_name=\"meta-llama/Meta-Llama-3-8B\",\n    max_seq_length=max_seq_length,\n    dtype=dtype,\n    load_in_4bit=load_in_4bit,\n    token=\"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\", # use one if using gated models like meta-llama/Llama-2-7b-hf\n)\nmodel = FastLanguageModel.get_peft_model(\n    model,\n    r=16, # Choose any number > 0 ! Suggested 8, 16, 32, 64, 128\n    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n                    \"gate_proj\", \"up_proj\", \"down_proj\"],\n    lora_alpha=16,\n    lora_dropout=0, # Supports any, but = 0 is optimized\n    bias=\"none\",    # Supports any, but = \"none\" is optimized\n    use_gradient_checkpointing=\"unsloth\", # True or \"unsloth\" for very long context\n    random_state=3407,\n    use_rslora=False,  # We support rank stabilized LoRA\n    loftq_config=None, # And LoftQ\n).to(device)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Format prompt","metadata":{}},{"cell_type":"code","source":"# Define prompt and EOS token\nalpaca_prompt = \"\"\"Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n\n### Instruction:\n{}\n\n### Input:\n{}\n\n### Response:\n{}\"\"\"\nEOS_TOKEN = tokenizer.eos_token\n\n# Formatting prompts function\ndef formatting_prompts_func(examples):\n    instructions = examples[\"instruction\"]\n    inputs = examples[\"input\"]\n    outputs = examples[\"output\"]\n    texts = []\n    for instruction, input, output in zip(instructions, inputs, outputs):\n        text = alpaca_prompt.format(instruction, input, output) + EOS_TOKEN\n        texts.append(text)\n    return {\"text\": texts}\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Load dataset, train finetune, save and log","metadata":{}},{"cell_type":"code","source":"\n# Load and format dataset\nsst5 = load_dataset(\"SetFit/sst5\", split=\"train\")  # Adjusted to SST5 dataset\nsst5 = sst5.map(lambda example: {\"label\": 2 if example[\"label\"] > 3 else (0 if example[\"label\"] < 2 else 1)})  # Grouping into 3 labels\n\n# Define training arguments\ntraining_args = TrainingArguments(\n    per_device_train_batch_size=2,\n    gradient_accumulation_steps=4,\n    warmup_steps=5,\n    max_steps=2,\n    learning_rate=2e-4,\n    fp16=torch.cuda.is_available() and not is_bfloat16_supported(),\n    bf16=is_bfloat16_supported(),\n    logging_steps=1,\n    optim=\"adamw_8bit\",\n    weight_decay=0.01,\n    lr_scheduler_type=\"linear\",\n    seed=3407,\n    output_dir=\"outputs\",\n)\n\n# Initialize trainer\ntrainer = SFTTrainer(\n    model=model,\n    tokenizer=tokenizer,\n    train_dataset=sst5,\n    dataset_text_field=\"text\",\n    max_seq_length=max_seq_length,\n    dataset_num_proc=2,\n    packing=False, # Can make training 5x faster for short sequences.\n    args=training_args,\n)\n\ntrainer.train()\n\n# Save the model\nmodel.save_pretrained(\"outputs/saved_model\")\ntokenizer.save_pretrained(\"outputs/saved_model\")\n\n\n# Log parameters and metrics\nmlflow.log_params({\n    \"per_device_train_batch_size\": training_args.per_device_train_batch_size,\n    \"gradient_accumulation_steps\": training_args.gradient_accumulation_steps,\n    \"warmup_steps\": training_args.warmup_steps,\n    \"max_steps\": training_args.max_steps,\n    \"learning_rate\": training_args.learning_rate,\n    \"fp16\": training_args.fp16,\n    \"bf16\": training_args.bf16,\n    \"logging_steps\": training_args.logging_steps,\n    \"weight_decay\": training_args.weight_decay,\n    \"lr_scheduler_type\": training_args.lr_scheduler_type,\n    \"seed\": training_args.seed,\n    \"output_dir\": training_args.output_dir,\n})\n\nmlflow.log_metric(\"final_loss\", trainer.state.log_history[-1]['train_loss'])\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Install required packages\n# !pip install -U mlflow datasets transformers peft unsloth trl evaluate torch --no-cache-dir\n\n# Import required libraries\nimport mlflow\nfrom unsloth import FastLanguageModel\nimport torch\nfrom datasets import load_dataset\nfrom transformers import TrainingArguments, AutoTokenizer\nfrom unsloth import is_bfloat16_supported\nfrom trl import SFTTrainer\nimport evaluate\nimport numpy as np\nfrom sklearn.metrics import confusion_matrix, classification_report\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport os\nimport shutil\n\n# Clear cache directory\ncache_dir = os.path.join(os.path.expanduser(\"~\"), \".cache\")\nif os.path.exists(cache_dir):\n    shutil.rmtree(cache_dir)\nos.makedirs(cache_dir)\n\n# Check if CUDA is available and set device accordingly\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n# Initialize MLflow\nmlflow.set_tracking_uri(\"file:///mnt/data/mlruns\")\nmlflow.set_experiment(\"unsloth-train\")\n\n# Set up model configuration\nmax_seq_length = 2048 # Choose any! We auto support RoPE Scaling internally!\ndtype = None # None for auto detection. Float16 for Tesla T4, V100, Bfloat16 for Ampere+\nload_in_4bit = True # Use 4bit quantization to reduce memory usage. Can be False.\n\n# 4bit pre-quantized models\nfourbit_models = [\n    \"unsloth/mistral-7b-v0.3-bnb-4bit\",\n    \"unsloth/mistral-7b-instruct-v0.3-bnb-4bit\",\n    \"unsloth/llama-3-8b-bnb-4bit\",\n    \"unsloth/llama-3-8b-Instruct-bnb-4bit\",\n    \"unsloth/llama-3-70b-bnb-4bit\",\n    \"unsloth/Phi-3-mini-4k-instruct\",\n    \"unsloth/Phi-3-medium-4k-instruct\",\n    \"unsloth/mistral-7b-bnb-4bit\",\n    \"unsloth/gemma-7b-bnb-4bit\",\n]\n\n# Load model and tokenizer\nmodel, tokenizer = FastLanguageModel.from_pretrained(\n    model_name=\"meta-llama/Meta-Llama-3-8B\",\n    max_seq_length=max_seq_length,\n    dtype=dtype,\n    load_in_4bit=load_in_4bit,\n    token=\"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\", # use one if using gated models like meta-llama/Llama-2-7b-hf\n)\nmodel = FastLanguageModel.get_peft_model(\n    model,\n    r=16, # Choose any number > 0 ! Suggested 8, 16, 32, 64, 128\n    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n                    \"gate_proj\", \"up_proj\", \"down_proj\"],\n    lora_alpha=16,\n    lora_dropout=0, # Supports any, but = 0 is optimized\n    bias=\"none\",    # Supports any, but = \"none\" is optimized\n    use_gradient_checkpointing=\"unsloth\", # True or \"unsloth\" for very long context\n    random_state=3407,\n    use_rslora=False,  # We support rank stabilized LoRA\n    loftq_config=None, # And LoftQ\n).to(device)\n\n# Define prompt and EOS token\nalpaca_prompt = \"\"\"Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n\n### Instruction:\n{}\n\n### Input:\n{}\n\n### Response:\n{}\"\"\"\nEOS_TOKEN = tokenizer.eos_token\n\n# Formatting prompts function\ndef formatting_prompts_func(examples):\n    instructions = examples[\"instruction\"]\n    inputs = examples[\"input\"]\n    outputs = examples[\"output\"]\n    texts = []\n    for instruction, input, output in zip(instructions, inputs, outputs):\n        text = alpaca_prompt.format(instruction, input, output) + EOS_TOKEN\n        texts.append(text)\n    return {\"text\": texts}\n\n# Load and format dataset\nsst5 = load_dataset(\"SetFit/sst5\", split=\"train\")  # Adjusted to SST5 dataset\nsst5 = sst5.map(lambda example: {\"label\": 2 if example[\"label\"] > 3 else (0 if example[\"label\"] < 2 else 1)})  # Grouping into 3 labels\n\n# Define training arguments\ntraining_args = TrainingArguments(\n    per_device_train_batch_size=2,\n    gradient_accumulation_steps=4,\n    warmup_steps=5,\n    max_steps=2,\n    learning_rate=2e-4,\n    fp16=torch.cuda.is_available() and not is_bfloat16_supported(),\n    bf16=is_bfloat16_supported(),\n    logging_steps=1,\n    optim=\"adamw_8bit\",\n    weight_decay=0.01,\n    lr_scheduler_type=\"linear\",\n    seed=3407,\n    output_dir=\"outputs\",\n)\n\n# Initialize trainer\ntrainer = SFTTrainer(\n    model=model,\n    tokenizer=tokenizer,\n    train_dataset=sst5,\n    dataset_text_field=\"text\",\n    max_seq_length=max_seq_length,\n    dataset_num_proc=2,\n    packing=False, # Can make training 5x faster for short sequences.\n    args=training_args,\n)\n\ntrainer.train()\n\n# Save the model\nmodel.save_pretrained(\"outputs/saved_model\")\ntokenizer.save_pretrained(\"outputs/saved_model\")\n\n# Log parameters and metrics\nmlflow.log_params({\n    \"per_device_train_batch_size\": training_args.per_device_train_batch_size,\n    \"gradient_accumulation_steps\": training_args.gradient_accumulation_steps,\n    \"warmup_steps\": training_args.warmup_steps,\n    \"max_steps\": training_args.max_steps,\n    \"learning_rate\": training_args.learning_rate,\n    \"fp16\": training_args.fp16,\n    \"bf16\": training_args.bf16,\n    \"logging_steps\": training_args.logging_steps,\n    \"weight_decay\": training_args.weight_decay,\n    \"lr_scheduler_type\": training_args.lr_scheduler_type,\n    \"seed\": training_args.seed,\n    \"output_dir\": training_args.output_dir,\n})\n\nmlflow.log_metric(\"final_loss\", trainer.state.log_history[-1]['train_loss'])\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Evaluate finetuned model","metadata":{}},{"cell_type":"code","source":"import accelerate\nfrom datasets import load_dataset\nfrom transformers import AutoTokenizer\nimport torch\nimport numpy as np\nfrom sklearn.metrics import confusion_matrix, classification_report\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Initialize the accelerator\naccelerator = accelerate.Accelerator()\n\n# Function to get predictions from vllm using batch processing\ndef get_vllm_predictions(model, tokenizer, dataset, batch_size=32):\n    model.eval()\n    predictions, labels = [], []\n    for i in range(0, len(dataset), batch_size):\n        batch = dataset[i:i+batch_size]\n        inputs = tokenizer(batch[\"text\"], return_tensors=\"pt\", max_length=max_seq_length, truncation=True, padding=True).to(device)\n        with torch.no_grad():\n            outputs = model(**inputs)\n        logits = outputs.logits\n        preds = torch.argmax(logits, dim=-1).cpu().numpy()\n        predictions.extend(preds)\n        labels.extend(batch[\"label\"])\n    return predictions, labels\n\n# Load tokenizer and model (assuming they are defined earlier in the script)\n# tokenizer = AutoTokenizer.from_pretrained(\"model_name\")\n# model = SomeModelClass.from_pretrained(\"model_name\")\n\n# Load validation and test sets and group into 3 labels\nval_set = load_dataset(\"SetFit/sst5\", split=\"validation\").map(lambda example: {\"label\": 2 if example[\"label\"] > 3 else (0 if example[\"label\"] < 2 else 1)})\ntest_set = load_dataset(\"SetFit/sst5\", split=\"test\").map(lambda example: {\"label\": 2 if example[\"label\"] > 3 else (0 if example[\"label\"] < 2 else 1)})\n\n# Ensure labels are integers\nval_set = val_set.map(lambda example: {\"label\": int(example[\"label\"])})\ntest_set = test_set.map(lambda example: {\"label\": int(example[\"label\"])})\n\n# Move model to the correct device\nmodel = accelerator.prepare(model)\n\n# Get predictions\nval_predictions, val_labels = get_vllm_predictions(model, tokenizer, val_set)\ntest_predictions, test_labels = get_vllm_predictions(model, tokenizer, test_set)\n\n# Ensure predictions and labels are numpy arrays of integer type\nval_predictions = np.array(val_predictions, dtype=int)\nval_labels = np.array(val_labels, dtype=int)\ntest_predictions = np.array(test_predictions, dtype=int)\ntest_labels = np.array(test_labels, dtype=int)\n\n# Check for consistency in the data types and lengths\nprint(\"Validation Labels Type:\", val_labels.dtype, \"Length:\", len(val_labels))\nprint(\"Validation Predictions Type:\", val_predictions.dtype, \"Length:\", len(val_predictions))\nprint(\"Test Labels Type:\", test_labels.dtype, \"Length:\", len(test_labels))\nprint(\"Test Predictions Type:\", test_predictions.dtype, \"Length:\", len(test_predictions))\n\n# Calculate confusion matrix and classification report\nval_conf_matrix = confusion_matrix(val_labels, val_predictions)\ntest_conf_matrix = confusion_matrix(test_labels, test_predictions)\n\n# Plot confusion matrix for validation set\nplt.figure(figsize=(10, 7))\nsns.heatmap(val_conf_matrix, annot=True, fmt='d', cmap='Blues', xticklabels=[\"Negative\", \"Neutral\", \"Positive\"], yticklabels=[\"Negative\", \"Neutral\", \"Positive\"])\nplt.xlabel('Predicted')\nplt.ylabel('Actual')\nplt.title('Validation Set Confusion Matrix')\nplt.show()\n\n# Plot confusion matrix for test set\nplt.figure(figsize=(10, 7))\nsns.heatmap(test_conf_matrix, annot=True, fmt='d', cmap='Blues', xticklabels=[\"Negative\", \"Neutral\", \"Positive\"], yticklabels=[\"Negative\", \"Neutral\", \"Positive\"])\nplt.xlabel('Predicted')\nplt.ylabel('Actual')\nplt.title('Test Set Confusion Matrix')\nplt.show()\n\n# Print classification reports\nprint(\"Validation Set Classification Report:\")\nprint(classification_report(val_labels, val_predictions, target_names=[\"Negative\", \"Neutral\", \"Positive\"]))\n\nprint(\"Test Set Classification Report:\")\nprint(classification_report(test_labels, test_predictions, target_names=[\"Negative\", \"Neutral\", \"Positive\"]))\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Check GPU usage","metadata":{}},{"cell_type":"code","source":"!nvidia-smi","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Clear cache","metadata":{}},{"cell_type":"code","source":"del trainer\ndel sst5\ntorch.cuda.empty_cache()\n\n!nvidia-smi","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Restart and delete gpu memory for using vllm for inference","metadata":{}},{"cell_type":"code","source":"from transformers import AutoTokenizer, AutoModelForCausalLM\nfrom peft import PeftModelForCausalLM\nimport os\n\n# Ensure the token is available (replace 'your_token' with your actual Hugging Face token)\ntoken = os.getenv('HF_TOKEN', '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')\n\n# Load tokenizer with authentication\ntokenizer = AutoTokenizer.from_pretrained(\"outputs\", use_auth_token=token)\n\n# Base model ID (should match the model you originally trained)\nbase_model_id = \"meta-llama/Meta-Llama-3-8B\"\n\n# Load the base model first with authentication\nbase_model = AutoModelForCausalLM.from_pretrained(base_model_id, use_auth_token=token)\n\n# Load the LoRA weights and apply to the base model\nmodel = PeftModelForCausalLM.from_pretrained(base_model, \"outputs\")\n\n# Print confirmation\nprint(\"Model and tokenizer loaded successfully.\")\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"break","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Save the model and tokenizer\nmodel.save_pretrained(\"outputs\")\ntokenizer.save_pretrained(\"outputs\")\n\n\nfrom transformers import AutoTokenizer, AutoModelForCausalLM\nfrom peft import PeftModelForCausalLM\n\n# Load tokenizer\ntokenizer = AutoTokenizer.from_pretrained(\"outputs\")\n\n# Base model ID (should match the model you originally trained)\nbase_model_id = \"meta-llama/Meta-Llama-3-8B\"\n\n# Load the base model first\nbase_model = AutoModelForCausalLM.from_pretrained(base_model_id)\n\n# Load the LoRA weights and apply to the base model\nmodel = PeftModelForCausalLM.from_pretrained(base_model, \"outputs\")\n\n# Print confirmation\nprint(\"Model and tokenizer loaded successfully.\")\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Save the model and tokenizer\nmodel.save_pretrained(\"outputs\")\ntokenizer.save_pretrained(\"outputs\")\n\nfrom transformers import AutoTokenizer\nfrom peft import PeftModelForCausalLM\n# Save the model and tokenizer\nmodel.save_pretrained(\"outputs\")\ntokenizer.save_pretrained(\"outputs\")\n\n# Load tokenizer\ntokenizer = AutoTokenizer.from_pretrained(\"outputs\")\n\n# Load model\nmodel = PeftModelForCausalLM.from_pretrained(\"outputs\")\n\n# Print confirmation\nprint(\"Model and tokenizer loaded successfully.\")\n\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"code","source":"import accelerate\nfrom datasets import load_dataset\nfrom transformers import AutoTokenizer\nimport torch\nimport numpy as np\nfrom sklearn.metrics import confusion_matrix, classification_report\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Initialize the accelerator\naccelerator = accelerate.Accelerator()\n\n# Function to get predictions from vllm using batch processing\ndef get_vllm_predictions(model, tokenizer, dataset, batch_size=32):\n    model.eval()\n    predictions, labels = [], []\n    for i in range(0, len(dataset), batch_size):\n        batch = dataset[i:i+batch_size]\n        inputs = tokenizer(batch[\"text\"], return_tensors=\"pt\", max_length=max_seq_length, truncation=True, padding=True).to(device)\n        with torch.no_grad():\n            outputs = model(**inputs)\n        logits = outputs.logits\n        preds = torch.argmax(logits, dim=-1).cpu().numpy()\n        predictions.extend(preds)\n        labels.extend(batch[\"label\"])\n    return predictions, labels\n\n# Load tokenizer and model (assuming they are defined earlier in the script)\n# tokenizer = AutoTokenizer.from_pretrained(\"model_name\")\n# model = SomeModelClass.from_pretrained(\"model_name\")\n\n# Load validation and test sets and group into 3 labels\nval_set = load_dataset(\"SetFit/sst5\", split=\"validation\").map(lambda example: {\"label\": 2 if example[\"label\"] > 3 else (0 if example[\"label\"] < 2 else 1)})\ntest_set = load_dataset(\"SetFit/sst5\", split=\"test\").map(lambda example: {\"label\": 2 if example[\"label\"] > 3 else (0 if example[\"label\"] < 2 else 1)})\n\n# Ensure labels are integers\nval_set = val_set.map(lambda example: {\"label\": int(example[\"label\"])})\ntest_set = test_set.map(lambda example: {\"label\": int(example[\"label\"])})\n\n# Move model to the correct device\nmodel = accelerator.prepare(model)\n\n# Get predictions\nval_predictions, val_labels = get_vllm_predictions(model, tokenizer, val_set)\ntest_predictions, test_labels = get_vllm_predictions(model, tokenizer, test_set)\n\n# Ensure predictions and labels are numpy arrays of integer type\nval_predictions = np.array(val_predictions, dtype=int)\nval_labels = np.array(val_labels, dtype=int)\ntest_predictions = np.array(test_predictions, dtype=int)\ntest_labels = np.array(test_labels, dtype=int)\n\n# Check for consistency in the data types and lengths\nprint(\"Validation Labels Type:\", val_labels.dtype, \"Length:\", len(val_labels))\nprint(\"Validation Predictions Type:\", val_predictions.dtype, \"Length:\", len(val_predictions))\nprint(\"Test Labels Type:\", test_labels.dtype, \"Length:\", len(test_labels))\nprint(\"Test Predictions Type:\", test_predictions.dtype, \"Length:\", len(test_predictions))\n\n# Calculate confusion matrix and classification report\nval_conf_matrix = confusion_matrix(val_labels, val_predictions)\ntest_conf_matrix = confusion_matrix(test_labels, test_predictions)\n\n# Plot confusion matrix for validation set\nplt.figure(figsize=(10, 7))\nsns.heatmap(val_conf_matrix, annot=True, fmt='d', cmap='Blues', xticklabels=[\"Negative\", \"Neutral\", \"Positive\"], yticklabels=[\"Negative\", \"Neutral\", \"Positive\"])\nplt.xlabel('Predicted')\nplt.ylabel('Actual')\nplt.title('Validation Set Confusion Matrix')\nplt.show()\n\n# Plot confusion matrix for test set\nplt.figure(figsize=(10, 7))\nsns.heatmap(test_conf_matrix, annot=True, fmt='d', cmap='Blues', xticklabels=[\"Negative\", \"Neutral\", \"Positive\"], yticklabels=[\"Negative\", \"Neutral\", \"Positive\"])\nplt.xlabel('Predicted')\nplt.ylabel('Actual')\nplt.title('Test Set Confusion Matrix')\nplt.show()\n\n# Print classification reports\nprint(\"Validation Set Classification Report:\")\nprint(classification_report(val_labels, val_predictions, target_names=[\"Negative\", \"Neutral\", \"Positive\"]))\n\nprint(\"Test Set Classification Report:\")\nprint(classification_report(test_labels, test_predictions, target_names=[\"Negative\", \"Neutral\", \"Positive\"]))\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"break","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"raw","source":"","metadata":{}},{"cell_type":"code","source":"!ls -a","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"val_predictions = np.array(val_predictions, dtype=int)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"val_predictions","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"val_set","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Function to get predictions from vllm using batch processing\ndef get_vllm_predictions(model, tokenizer, dataset, batch_size=32):\n    model.eval()\n    predictions, labels = [], []\n    for i in range(0, len(dataset), batch_size):\n        batch = dataset[i:i+batch_size]\n        inputs = tokenizer(batch[\"text\"], return_tensors=\"pt\", max_length=max_seq_length, truncation=True, padding=True).to(device)\n        with torch.no_grad():\n            outputs = model(**inputs)\n        logits = outputs.logits\n        preds = torch.argmax(logits, dim=-1).cpu().numpy()\n        predictions.extend(preds)\n        labels.extend(batch[\"label\"])\n    return predictions, labels\n\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Save the model and tokenizer\nmodel.save_pretrained(\"outputs\")\ntokenizer.save_pretrained(\"outputs\")\n\n# Verify the contents of the outputs directory\nimport os\nprint(os.listdir(\"outputs\"))\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from transformers import AutoTokenizer, AutoModelForCausalLM\nfrom peft import PeftModelForCausalLM\n\n# Load tokenizer\ntokenizer = AutoTokenizer.from_pretrained(\"outputs\")\n\n# Load model\nmodel = PeftModelForCausalLM.from_pretrained(\"outputs\")\n\n# Print confirmation\nprint(\"Model and tokenizer loaded successfully.\")\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Save the model and tokenizer\nmodel.save_pretrained(\"outputs\")\ntokenizer.save_pretrained(\"outputs\")\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from transformers import AutoTokenizer, AutoModelForCausalLM\nfrom peft import PeftModelForCausalLM\n\n# Load tokenizer\ntokenizer = AutoTokenizer.from_pretrained(\"outputs\")\n\n# Load model\nmodel = PeftModelForCausalLM.from_pretrained(\"outputs\")\n\n# Print confirmation\nprint(\"Model and tokenizer loaded successfully.\")\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Load tokenizer and model (assuming they are defined earlier in the script)\ntokenizer = AutoTokenizer.from_pretrained(\"outputs/model\")\nmodel = SomeModelClass.from_pretrained(\"outputs/model\")\n\n# Load validation and test sets and group into 3 labels\nval_set = load_dataset(\"SetFit/sst5\", split=\"validation\").map(lambda example: {\"label\": 2 if example[\"label\"] > 3 else (0 if example[\"label\"] < 2 else 1)})\ntest_set = load_dataset(\"SetFit/sst5\", split=\"test\").map(lambda example: {\"label\": 2 if example[\"label\"] > 3 else (0 if example[\"label\"] < 2 else 1)})\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\n\n\n# Ensure labels are integers\nval_set = val_set.map(lambda example: {\"label\": int(example[\"label\"])})\ntest_set = test_set.map(lambda example: {\"label\": int(example[\"label\"])})\n\n# Move model to the correct device\nmodel = accelerator.prepare(model)\n\n# Get predictions\nval_predictions, val_labels = get_vllm_predictions(model, tokenizer, val_set)\ntest_predictions, test_labels = get_vllm_predictions(model, tokenizer, test_set)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}